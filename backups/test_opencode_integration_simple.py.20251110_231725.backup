from functools import lru_cache
'\ntest_opencode_integration_simple.py - Simple integration tests for Opencode compatibility\n\nThis simplified test suite focuses on core Opencode integration without\nrequiring TUI dependencies that may not be available in the test environment.\n\nTest Coverage:\n- Configuration integration\n- LLM client functionality\n- Brain integration\n- Skills system\n- Model translation\n- Basic functionality validation\n'
import unittest
import json
import tempfile
import os
import sys
from pathlib import Path
from unittest.mock import patch, MagicMock
from typing import Dict, List, Any
sys.path.insert(0, str(Path(__file__).parent))
from config_opencode import Config, OpencodeConfig, load_config, find_opencode_config, read_opencode_config, translate_opencode_model_to_neo, get_current_opencode_model, is_opencode_available
from llm_client_opencode import LLMClient, OpencodeLLMClient, LLMResponse
from brain_opencode import OpencodeBrain, ConversationHistory
from skills import SkillRegistry

class TestOpencodeConfigSimple(unittest.TestCase):
    """Test Opencode configuration integration - simplified"""

    def setUp(self):
        """Set up test environment"""
        self.temp_dir = tempfile.mkdtemp()
        self.config_path = os.path.join(self.temp_dir, 'opencode.json')

    def tearDown(self):
        """Clean up test environment"""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def test_find_opencode_config(self):
        """Test finding Opencode config file"""
        with open(self.config_path, 'w') as f:
            json.dump({'model': 'openai/gpt-3.5-turbo'}, f)
        result = find_opencode_config()
        self.assertEqual(result, self.config_path)

    def test_read_opencode_config(self):
        """Test reading Opencode configuration"""
        config_data = {'model': 'anthropic/claude-3-sonnet', 'provider_options': {'temperature': 0.7}}
        with open(self.config_path, 'w') as f:
            json.dump(config_data, f)
        result = read_opencode_config(self.config_path)
        self.assertIsNotNone(result)
        self.assertEqual(result.model, 'anthropic/claude-3-sonnet')
        self.assertEqual(result.provider_options['temperature'], 0.7)

    def test_translate_opencode_model_to_neo(self):
        """Test model format translation"""
        (provider, model) = translate_opencode_model_to_neo('openai/gpt-4')
        self.assertEqual(provider, 'api')
        self.assertEqual(model, 'gpt-4')
        (provider, model) = translate_opencode_model_to_neo('ollama/llama2')
        self.assertEqual(provider, 'ollama')
        self.assertEqual(model, 'llama2')
        (provider, model) = translate_opencode_model_to_neo('invalid')
        self.assertEqual(provider, 'ollama')
        self.assertEqual(model, 'ggml-neural-chat')

class TestLLMClientSimple(unittest.TestCase):
    """Test Opencode-compatible LLM client - simplified"""

    def setUp(self):
        """Set up test environment"""
        self.config = Config(provider='ollama', model_name='test-model', api_endpoint='http://localhost:11434')

    def test_client_initialization(self):
        """Test LLM client initialization"""
        client = LLMClient(self.config)
        self.assertEqual(client.cfg, self.config)
        self.assertIsNotNone(client.session)

    def test_model_operations(self):
        """Test model setting and retrieval"""
        client = LLMClient(self.config)
        test_model = 'openai/gpt-3.5-turbo'
        client.set_model(test_model)
        self.assertEqual(client.get_current_model(), test_model)
        models = client.get_available_models()
        self.assertIsInstance(models, list)

class TestBrainSimple(unittest.TestCase):
    """Test Opencode brain integration - simplified"""

    def setUp(self):
        """Set up test environment"""
        self.config = Config(provider='ollama', model_name='test-model')
        self.skills = SkillRegistry()
        self.brain = OpencodeBrain(self.config, self.skills)

    def test_brain_initialization(self):
        """Test brain initialization with Opencode integration"""
        self.assertEqual(self.brain.cfg, self.config)
        self.assertIsInstance(self.brain.history, ConversationHistory)
        self.assertIsNotNone(self.brain.llm)

    def test_intent_parsing(self):
        """Test intent parsing including model switching"""
        result = self.brain.parse_intent('/model gpt-3.5-turbo')
        self.assertEqual(result['intent'], 'model_switch')
        self.assertEqual(result['action'], 'switch_model')
        result = self.brain.parse_intent('analyze this text')
        self.assertEqual(result['intent'], 'skill')
        self.assertEqual(result['skill'], 'text_analysis')
        result = self.brain.parse_intent('minimax analyze this')
        self.assertEqual(result['intent'], 'minimax')
        self.assertEqual(result['skill'], 'minimax_agent')

    def test_model_switching(self):
        """Test model switching functionality"""
        result = self.brain.switch_model('openai/gpt-3.5-turbo')
        self.assertIn('‚úÖ Model switched', result)
        self.assertEqual(self.brain.llm.get_current_model(), 'openai/gpt-3.5-turbo')

    def test_status_reporting(self):
        """Test brain status reporting"""
        status = self.brain.get_status()
        self.assertIn('current_model', status)
        self.assertIn('provider', status)
        self.assertIn('available_skills', status)
        self.assertIn('config_source', status)

class TestSkillsIntegrationSimple(unittest.TestCase):
    """Test skills system integration with Opencode - simplified"""

    def setUp(self):
        """Set up test environment"""
        self.config = Config(provider='ollama', model_name='test-model')
        self.skills = SkillRegistry()
        self.brain = OpencodeBrain(self.config, self.skills)

    def test_skill_discovery(self):
        """Test that skills are properly discovered"""
        available_skills = list(self.skills._skills.keys())
        self.assertIn('minimax_agent', available_skills)
        self.assertIn('text_analysis', available_skills)
        self.assertIn('code_generation', available_skills)

    def test_skill_execution(self):
        """Test skill execution through brain"""
        result = self.brain.route_to_skill('minimax_agent', 'analyze test')
        self.assertIn('chosen_skill', result)
        self.assertEqual(result['chosen_skill'], 'minimax_agent')
        self.assertIn('output', result)

class TestBackwardCompatibilitySimple(unittest.TestCase):
    """Test backward compatibility with original Neo-Clone - simplified"""

    def setUp(self):
        """Set up test environment"""
        self.config = Config(provider='ollama', model_name='test-model')

    def test_config_compatibility(self):
        """Test that config still works as before"""
        self.assertEqual(self.config.provider, 'ollama')
        self.assertEqual(self.config.model_name, 'test-model')
        self.assertEqual(self.config.temperature, 0.2)

    def test_brain_compatibility(self):
        """Test brain backward compatibility"""
        skills = SkillRegistry()
        from brain import Brain
        brain = Brain(self.config, skills)
        self.assertIsNotNone(brain)

    def test_llm_client_compatibility(self):
        """Test LLM client backward compatibility"""
        from brain import LLMClient
        client = LLMClient(self.config)
        self.assertIsNotNone(client)

def create_sample_queries():
    """Create sample queries for testing"""
    return [{'query': "analyze the sentiment of this text: 'I love this product!'", 'expected_intent': 'skill', 'expected_skill': 'text_analysis'}, {'query': '/model openai/gpt-3.5-turbo', 'expected_intent': 'model_switch', 'expected_action': 'switch_model'}, {'query': 'minimax analyze the intent of user requests', 'expected_intent': 'minimax', 'expected_skill': 'minimax_agent'}, {'query': 'generate python code to sort a list', 'expected_intent': 'skill', 'expected_skill': 'code_generation'}, {'query': 'search for information about AI', 'expected_intent': 'skill', 'expected_skill': 'web_search'}]

@lru_cache(maxsize=128)
def run_integration_tests():
    """Run integration tests"""
    test_queries = create_sample_queries()
    config = Config(provider='ollama', model_name='test-model')
    skills = SkillRegistry()
    brain = OpencodeBrain(config, skills)
    print('üß™ Running Integration Tests...')
    print('=' * 50)
    passed_tests = 0
    total_tests = len(test_queries)
    for (i, test_case) in enumerate(test_queries, 1):
        print(f"\nüìù Test {i}: {test_case['query']}")
        try:
            intent = brain.parse_intent(test_case['query'])
            print(f'   Intent: {intent}')
            expected_intent = test_case['expected_intent']
            if intent['intent'] == expected_intent:
                print(f'   ‚úÖ Intent validation passed')
                passed_tests += 1
            else:
                print(f"   ‚ùå Intent validation failed: expected {expected_intent}, got {intent['intent']}")
            if intent['intent'] == 'model_switch':
                result = brain.handle_model_switch(test_case['query'])
                print(f'   Model switch result: {result}')
            elif intent['intent'] == 'skill':
                result = brain.route_to_skill(intent['skill'], test_case['query'])
                print(f"   Skill result: {result.get('output', 'No output')[:100]}...")
            elif intent['intent'] == 'minimax':
                result = brain.route_to_skill(intent['skill'], test_case['query'])
                print(f"   MiniMax result: {result.get('output', 'No output')[:100]}...")
        except Exception as e:
            print(f'   ‚ùå Test failed with error: {str(e)}')
    print('\n' + '=' * 50)
    print(f'üéâ Integration tests completed! Passed: {passed_tests}/{total_tests}')
    return (passed_tests, total_tests)

def main():
    """Main test runner"""
    print('üöÄ Starting Opencode Integration Tests (Simplified)')
    print('=' * 60)
    print('\nüß™ Running Unit Tests...')
    unittest.main(argv=[''], exit=False, verbosity=1)
    print('\nüîó Running Integration Tests...')
    (passed, total) = run_integration_tests()
    print('\nüìä Test Results Summary:')
    print(f'   Unit Tests: Multiple test cases')
    print(f'   Integration Tests: {passed}/{total} passed')
    print('\n‚úÖ All tests completed!')
if __name__ == '__main__':
    main()