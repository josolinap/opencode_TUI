from functools import lru_cache
'\ncomprehensive_model_validator.py - Validates all available models and removes non-working ones\n\nThis script tests every model available to Neo-Clone and only keeps the ones\nthat actually work, removing broken models from the available list.\n'
import logging
import time
import json
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from llm_client_opencode import OpencodeLLMClient
from config_opencode import load_config
logger = logging.getLogger(__name__)

@dataclass
class ModelValidationResult:
    """Result of validating a single model"""
    name: str
    working: bool
    response_time: float
    error_message: str = ''
    test_response: str = ''
    quality_score: float = 0.0

class ComprehensiveModelValidator:
    """Validates all available models and filters out non-working ones"""

    def __init__(self):
        self.config = load_config()
        self.llm_client = OpencodeLLMClient(self.config)
        self.validation_results = []

    def validate_all_models(self) -> Dict[str, Any]:
        """Validate all available models"""
        print('COMPREHENSIVE MODEL VALIDATION')
        print('=' * 60)
        all_models = self.llm_client.get_available_models()
        print(f'Found {len(all_models)} models to validate')
        working_models = []
        failed_models = []
        for (i, model) in enumerate(all_models, 1):
            print(f'\n[{i}/{len(all_models)}] Testing: {model}')
            result = self._test_single_model(model)
            self.validation_results.append(result)
            if result.working:
                working_models.append(model)
                print(f'   WORKING - {result.response_time:.2f}s - Quality: {result.quality_score:.2f}')
            else:
                failed_models.append(model)
                print(f'   FAILED - {result.error_message}')
        summary = {'total_models': len(all_models), 'working_models': working_models, 'failed_models': failed_models, 'working_count': len(working_models), 'failed_count': len(failed_models), 'success_rate': len(working_models) / len(all_models) * 100, 'validation_timestamp': time.time(), 'detailed_results': [{'name': r.name, 'working': r.working, 'response_time': r.response_time, 'error_message': r.error_message, 'quality_score': r.quality_score} for r in self.validation_results]}
        self._print_validation_summary(summary)
        self._save_validated_models(working_models)
        return summary

    @lru_cache(maxsize=128)
    def _test_single_model(self, model: str) -> ModelValidationResult:
        """Test a single model with multiple test cases"""
        test_cases = [('What is 2+2?', 'math_logic'), ('Say hello', 'general_query'), ('Write \'print("hello")\' in one line', 'code_generation')]
        success_count = 0
        total_time = 0
        responses = []
        errors = []
        for (test_prompt, test_type) in test_cases:
            try:
                self.llm_client.set_model(model)
                start_time = time.time()
                messages = [{'role': 'user', 'content': test_prompt}]
                response = self.llm_client.chat(messages, timeout=15)
                response_time = time.time() - start_time
                if response.content and (not response.content.startswith('[')) and (len(response.content.strip()) > 0):
                    success_count += 1
                    total_time += response_time
                    responses.append(response.content)
                    quality = self._assess_response_quality(response.content, test_type)
                else:
                    errors.append(f'Invalid response: {response.content[:100]}')
            except Exception as e:
                errors.append(str(e))
        avg_time = total_time / max(1, success_count)
        success_rate = success_count / len(test_cases)
        is_working = success_rate >= 0.67
        quality_score = min(1.0, success_rate * (1.0 - min(1.0, avg_time / 20.0)))
        return ModelValidationResult(name=model, working=is_working, response_time=avg_time, error_message='; '.join(errors) if errors else '', test_response=responses[0] if responses else '', quality_score=quality_score)

    def _assess_response_quality(self, response: str, test_type: str) -> float:
        """Quick quality assessment of response"""
        if not response or response.startswith('['):
            return 0.0
        response_lower = response.lower().strip()
        length_score = min(1.0, len(response) / 10.0)
        if test_type == 'math_logic':
            has_number = any((char.isdigit() for char in response))
            return 0.8 if has_number else 0.3
        elif test_type == 'general_query':
            greeting_words = ['hello', 'hi', 'hey', 'greetings']
            has_greeting = any((word in response_lower for word in greeting_words))
            return 0.8 if has_greeting else 0.5
        elif test_type == 'code_generation':
            code_indicators = ['print', 'def', 'function', '()', '[]', '{}']
            has_code = any((indicator in response_lower for indicator in code_indicators))
            return 0.8 if has_code else 0.4
        return length_score * 0.6

    def _print_validation_summary(self, summary: Dict[str, Any]):
        """Print validation summary"""
        print('\n' + '=' * 60)
        print('VALIDATION SUMMARY')
        print('=' * 60)
        print(f"Total Models Tested: {summary['total_models']}")
        print(f"Working Models: {summary['working_count']}")
        print(f"Failed Models: {summary['failed_count']}")
        print(f"Success Rate: {summary['success_rate']:.1f}%")
        print(f"\nVALIDATED MODELS ({summary['working_count']}):")
        print('-' * 40)
        working_with_quality = []
        for model_name in summary['working_models']:
            result = next((r for r in self.validation_results if r.name == model_name))
            working_with_quality.append((model_name, result.quality_score, result.response_time))
        working_with_quality.sort(key=lambda x: (-x[1], x[2]))
        for (i, (model, quality, response_time)) in enumerate(working_with_quality, 1):
            print(f'{i:2d}. {model}')
            print(f'     Quality: {quality:.2f}')
            print(f'     Speed: {response_time:.2f}s')
        if summary['failed_models']:
            print(f"\nREMOVED MODELS ({summary['failed_count']}):")
            print('-' * 40)
            for model in summary['failed_models'][:10]:
                result = next((r for r in self.validation_results if r.name == model))
                print(f'   • {model} - {result.error_message[:50]}...')
            if len(summary['failed_models']) > 10:
                print(f"   ... and {len(summary['failed_models']) - 10} more")

    def _save_validated_models(self, working_models: List[str]):
        """Save the validated models list"""
        validated_data = {'validated_models': working_models, 'validation_timestamp': time.time(), 'total_validated': len(working_models), 'validation_results': [{'name': r.name, 'working': r.working, 'response_time': r.response_time, 'quality_score': r.quality_score, 'error_message': r.error_message} for r in self.validation_results]}
        with open('validated_models.json', 'w') as f:
            json.dump(validated_data, f, indent=2)
        with open('validated_models.py', 'w') as f:
            f.write('"""')
            f.write('Validated models list for Neo-Clone')
            f.write('Generated by comprehensive model validation')
            f.write('"""\n\n')
            f.write('VALIDATED_MODELS = ')
            f.write(json.dumps(working_models, indent=4))
            f.write('\n')
            f.write('VALIDATION_TIMESTAMP = ')
            f.write(str(time.time()))
            f.write('\n')
        print(f'\nSaved validated models to:')
        print(f'   • validated_models.json')
        print(f'   • validated_models.py')
        print(f'\nNeo-Clone will now only use these {len(working_models)} validated models!')

def main():
    logging.basicConfig(level=logging.INFO)
    validator = ComprehensiveModelValidator()
    summary = validator.validate_all_models()
    print(f'\nVALIDATION COMPLETE!')
    print(f"{summary['working_count']} models ready for use")
    print(f"{summary['failed_count']} models removed")
    print(f"{summary['success_rate']:.1f}% success rate")
if __name__ == '__main__':
    main()