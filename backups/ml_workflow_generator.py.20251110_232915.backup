from functools import lru_cache
'\nml_workflow_generator.py - Autonomous ML Model Building Workflow\n\nCreates and executes complete machine learning model building workflows with 5 automated steps:\n1. Data Analysis & Preprocessing\n2. Feature Engineering & Selection\n3. Model Selection & Architecture Design\n4. Training & Hyperparameter Optimization\n5. Evaluation & Deployment Preparation\n\nIntegrates with existing skills and task orchestrator for parallel execution.\n'
import json
import time
import logging
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from skills import BaseSkill
try:
    from skills.task_orchestrator import ModelOrchestrator, Task, TaskResult
except ImportError:
    ModelOrchestrator = None
    Task = None
    TaskResult = None
try:
    from config_opencode import load_config
except ImportError:

    def load_config():
        return {'models': {'default': 'opencode/big-pickle'}}
logger = logging.getLogger(__name__)

@dataclass
class MLWorkflowConfig:
    """Configuration for ML workflow"""
    project_name: str
    problem_type: str
    data_source: str
    target_variable: Optional[str] = None
    evaluation_metrics: List[str] = None
    deployment_target: str = 'local'
    optimization_priority: str = 'accuracy'

    def __post_init__(self):
        if self.evaluation_metrics is None:
            self.evaluation_metrics = ['accuracy'] if self.problem_type == 'classification' else ['mse', 'r2']

@dataclass
class WorkflowStep:
    """Represents a single step in the ML workflow"""
    step_id: str
    name: str
    description: str
    dependencies: List[str]
    estimated_duration: float
    required_skills: List[str]
    output_artifacts: List[str]

class MLWorkflowGenerator(BaseSkill):
    """Generates and executes complete ML model building workflows"""

    def __init__(self):
        self.config = load_config()
        self.orchestrator = ModelOrchestrator(self.config)
        self.workflow_templates = self._initialize_workflow_templates()

    @property
    def name(self) -> str:
        return 'ml_workflow_generator'

    @property
    def description(self) -> str:
        return 'Creates and executes complete ML model building workflows with 5 automated steps'

    @property
    def example_usage(self) -> str:
        return 'ml_workflow create --project "customer_churn" --type classification --data "customer_data.csv"'

    def execute(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Execute ML workflow generation and execution"""
        action = params.get('action', 'create')
        if action == 'create':
            return self._create_workflow(params)
        elif action == 'execute':
            return self._execute_workflow(params)
        elif action == 'monitor':
            return self._monitor_workflow(params)
        elif action == 'templates':
            return self._list_templates()
        else:
            return self._create_workflow(params)

    def _initialize_workflow_templates(self) -> Dict[str, List[WorkflowStep]]:
        """Initialize predefined workflow templates"""
        return {'classification': [WorkflowStep(step_id='data_analysis', name='Data Analysis & Preprocessing', description='Analyze data quality, handle missing values, perform initial EDA', dependencies=[], estimated_duration=5.0, required_skills=['data_inspector', 'analytics_analyzer'], output_artifacts=['cleaned_data.csv', 'data_analysis_report.json']), WorkflowStep(step_id='feature_engineering', name='Feature Engineering & Selection', description='Create new features, select relevant features, encode categorical variables', dependencies=['data_analysis'], estimated_duration=8.0, required_skills=['code_generation', 'analytics_analyzer'], output_artifacts=['feature_matrix.csv', 'feature_importance.json']), WorkflowStep(step_id='model_design', name='Model Selection & Architecture Design', description='Select appropriate models, design architecture, prepare training pipeline', dependencies=['feature_engineering'], estimated_duration=6.0, required_skills=['ml_training', 'code_generation'], output_artifacts=['model_config.json', 'training_pipeline.py']), WorkflowStep(step_id='training_optimization', name='Training & Hyperparameter Optimization', description='Train models, optimize hyperparameters, perform cross-validation', dependencies=['model_design'], estimated_duration=15.0, required_skills=['ml_training', 'task_orchestrator'], output_artifacts=['trained_model.pkl', 'hyperparameters.json', 'cv_results.json']), WorkflowStep(step_id='evaluation_deployment', name='Evaluation & Deployment Preparation', description='Evaluate model performance, prepare deployment artifacts, generate documentation', dependencies=['training_optimization'], estimated_duration=7.0, required_skills=['analytics_analyzer', 'code_generation'], output_artifacts=['evaluation_report.json', 'deployment_package.zip', 'model_documentation.md'])], 'regression': [WorkflowStep(step_id='data_analysis', name='Data Analysis & Preprocessing', description='Analyze data distribution, handle outliers, prepare target variable', dependencies=[], estimated_duration=5.0, required_skills=['data_inspector', 'analytics_analyzer'], output_artifacts=['cleaned_data.csv', 'target_analysis.json']), WorkflowStep(step_id='feature_engineering', name='Feature Engineering & Selection', description='Create polynomial features, select predictors, handle multicollinearity', dependencies=['data_analysis'], estimated_duration=8.0, required_skills=['code_generation', 'analytics_analyzer'], output_artifacts=['feature_matrix.csv', 'correlation_analysis.json']), WorkflowStep(step_id='model_design', name='Model Selection & Architecture Design', description='Compare regression models, design ensemble methods', dependencies=['feature_engineering'], estimated_duration=6.0, required_skills=['ml_training', 'code_generation'], output_artifacts=['model_comparison.json', 'ensemble_config.json']), WorkflowStep(step_id='training_optimization', name='Training & Hyperparameter Optimization', description='Train regression models, optimize for MSE/R2, validate assumptions', dependencies=['model_design'], estimated_duration=15.0, required_skills=['ml_training', 'task_orchestrator'], output_artifacts=['trained_models.pkl', 'residual_analysis.json', 'validation_results.json']), WorkflowStep(step_id='evaluation_deployment', name='Evaluation & Deployment Preparation', description='Evaluate predictions, prepare API endpoints, generate performance reports', dependencies=['training_optimization'], estimated_duration=7.0, required_skills=['analytics_analyzer', 'code_generation'], output_artifacts=['performance_report.json', 'prediction_api.py', 'model_summary.md'])]}

    def _create_workflow(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Create a new ML workflow based on parameters"""
        project_name = params.get('project_name', 'ml_project')
        problem_type = params.get('problem_type', 'classification')
        data_source = params.get('data_source', '')
        target_variable = params.get('target_variable')
        config = MLWorkflowConfig(project_name=project_name, problem_type=problem_type, data_source=data_source, target_variable=target_variable, evaluation_metrics=params.get('evaluation_metrics'), deployment_target=params.get('deployment_target', 'local'), optimization_priority=params.get('optimization_priority', 'accuracy'))
        steps = self.workflow_templates.get(problem_type, self.workflow_templates['classification'])
        workflow_plan = {'workflow_id': f'ml_wf_{int(time.time())}', 'config': asdict(config), 'steps': [asdict(step) for step in steps], 'estimated_total_duration': sum((step.estimated_duration for step in steps)), 'created_at': datetime.now().isoformat(), 'status': 'ready'}
        workflow_file = Path(f'{project_name}_workflow.json')
        with open(workflow_file, 'w') as f:
            json.dump(workflow_plan, f, indent=2)
        return {'status': 'success', 'message': f'Created ML workflow for {project_name}', 'workflow_id': workflow_plan['workflow_id'], 'workflow_file': str(workflow_file), 'steps_count': len(steps), 'estimated_duration': workflow_plan['estimated_total_duration'], 'next_action': 'Execute with: ml_workflow execute --workflow_id ' + workflow_plan['workflow_id']}

    @lru_cache(maxsize=128)
    def _execute_workflow(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a predefined ML workflow"""
        workflow_id = params.get('workflow_id')
        if not workflow_id:
            return {'status': 'error', 'message': 'workflow_id required'}
        workflow_file = Path(f"ml_wf_{workflow_id.split('_')[-1]}.json")
        if not workflow_file.exists():
            return {'status': 'error', 'message': f'Workflow file {workflow_file} not found'}
        with open(workflow_file, 'r') as f:
            workflow_plan = json.load(f)
        steps = []
        for step_data in workflow_plan['steps']:
            steps.append(WorkflowStep(**step_data))
        tasks = []
        for step in steps:
            task = Task(id=step.step_id, description=step.name, prompt=self._generate_step_prompt(step, workflow_plan['config']), dependencies=step.dependencies, priority=2, context={'step': asdict(step), 'config': workflow_plan['config']})
            tasks.append(task)
        logger.info(f'Executing ML workflow {workflow_id} with {len(tasks)} steps')
        results = self.orchestrator.execute_sequential_with_dependencies(tasks)
        successful_steps = [r for r in results if r.success]
        failed_steps = [r for r in results if not r.success]
        execution_summary = {'workflow_id': workflow_id, 'execution_time': sum((r.execution_time for r in results)), 'total_steps': len(tasks), 'successful_steps': len(successful_steps), 'failed_steps': len(failed_steps), 'success_rate': len(successful_steps) / len(tasks) if tasks else 0, 'results': [{'step_id': r.task_id, 'model_used': r.model_used, 'execution_time': r.execution_time, 'success': r.success, 'error': r.error_message} for r in results], 'artifacts': self._collect_artifacts(successful_steps, workflow_plan['config']), 'status': 'completed' if len(failed_steps) == 0 else 'partial', 'completed_at': datetime.now().isoformat()}
        results_file = Path(f'{workflow_id}_results.json')
        with open(results_file, 'w') as f:
            json.dump(execution_summary, f, indent=2)
        return {'status': 'success', 'message': f'Workflow execution completed with {len(successful_steps)}/{len(tasks)} steps successful', 'workflow_id': workflow_id, 'execution_summary': execution_summary, 'results_file': str(results_file)}

    def _generate_step_prompt(self, step: WorkflowStep, config: Dict[str, Any]) -> str:
        """Generate detailed prompt for each workflow step"""
        base_prompt = f"\n        ML Workflow Step: {step.name}\n        Project: {config['project_name']}\n        Problem Type: {config['problem_type']}\n        Data Source: {config['data_source']}\n        Target Variable: {config.get('target_variable', 'N/A')}\n        \n        Task: {step.description}\n        \n        Expected Outputs: {', '.join(step.output_artifacts)}\n        \n        Please provide:\n        1. Detailed analysis and implementation\n        2. Code snippets for the required operations\n        3. Configuration files and parameters\n        4. Quality checks and validation steps\n        5. Documentation for the artifacts produced\n        \n        Focus on {config['optimization_priority']} and ensure reproducibility.\n        "
        if step.step_id == 'data_analysis':
            base_prompt += '\n            \n            Specific requirements for data analysis:\n            - Load and inspect the dataset\n            - Identify data quality issues (missing values, outliers, duplicates)\n            - Generate descriptive statistics and visualizations\n            - Analyze target variable distribution\n            - Provide data cleaning recommendations\n            - Save cleaned dataset and analysis report\n            '
        elif step.step_id == 'feature_engineering':
            base_prompt += '\n            \n            Specific requirements for feature engineering:\n            - Create new meaningful features from existing data\n            - Handle categorical variables (encoding, one-hot, etc.)\n            - Perform feature selection using statistical methods\n            - Scale/normalize features as needed\n            - Document feature importance and correlations\n            - Save final feature matrix and feature documentation\n            '
        elif step.step_id == 'model_design':
            base_prompt += '\n            \n            Specific requirements for model design:\n            - Compare multiple model architectures suitable for the problem\n            - Design training and validation pipelines\n            - Set up cross-validation strategy\n            - Define hyperparameter search space\n            - Create model configuration files\n            - Prepare data loading and preprocessing pipelines\n            '
        elif step.step_id == 'training_optimization':
            base_prompt += '\n            \n            Specific requirements for training and optimization:\n            - Implement model training with cross-validation\n            - Perform hyperparameter optimization (grid search, random search, or Bayesian)\n            - Monitor training progress and prevent overfitting\n            - Compare model performance across different configurations\n            - Save best models and training logs\n            - Generate validation results and performance metrics\n            '
        elif step.step_id == 'evaluation_deployment':
            base_prompt += '\n            \n            Specific requirements for evaluation and deployment:\n            - Evaluate final model on test set with specified metrics\n            - Generate comprehensive performance report\n            - Create model documentation and usage examples\n            - Prepare deployment package (model files, requirements, API)\n            - Set up monitoring and logging for production\n            - Create user guide and technical documentation\n            '
        return base_prompt

    def _collect_artifacts(self, successful_results: List[TaskResult], config: Dict[str, Any]) -> List[str]:
        """Collect and list all artifacts generated during workflow execution"""
        artifacts = []
        if config['problem_type'] == 'classification':
            artifacts.extend(['cleaned_data.csv', 'data_analysis_report.json', 'feature_matrix.csv', 'feature_importance.json', 'model_config.json', 'training_pipeline.py', 'trained_model.pkl', 'hyperparameters.json', 'cv_results.json', 'evaluation_report.json', 'deployment_package.zip', 'model_documentation.md'])
        elif config['problem_type'] == 'regression':
            artifacts.extend(['cleaned_data.csv', 'target_analysis.json', 'feature_matrix.csv', 'correlation_analysis.json', 'model_comparison.json', 'ensemble_config.json', 'trained_models.pkl', 'residual_analysis.json', 'validation_results.json', 'performance_report.json', 'prediction_api.py', 'model_summary.md'])
        return artifacts

    def _monitor_workflow(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Monitor workflow execution status"""
        workflow_id = params.get('workflow_id')
        if not workflow_id:
            return {'status': 'error', 'message': 'workflow_id required'}
        results_file = Path(f'{workflow_id}_results.json')
        if not results_file.exists():
            return {'status': 'error', 'message': f'Results file for {workflow_id} not found'}
        with open(results_file, 'r') as f:
            results = json.load(f)
        return {'status': 'success', 'workflow_id': workflow_id, 'current_status': results['status'], 'progress': f"{results['successful_steps']}/{results['total_steps']} steps completed", 'execution_time': results['execution_time'], 'success_rate': results['success_rate'], 'artifacts_generated': len(results['artifacts']), 'last_updated': results['completed_at']}

    def _list_templates(self) -> Dict[str, Any]:
        """List available workflow templates"""
        templates = {}
        for (problem_type, steps) in self.workflow_templates.items():
            templates[problem_type] = {'steps_count': len(steps), 'estimated_duration': sum((step.estimated_duration for step in steps)), 'steps': [{'id': step.step_id, 'name': step.name} for step in steps]}
        return {'status': 'success', 'available_templates': templates, 'total_templates': len(templates)}
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    workflow_gen = MLWorkflowGenerator()
    print('Creating classification workflow...')
    create_result = workflow_gen.execute({'action': 'create', 'project_name': 'customer_churn_prediction', 'problem_type': 'classification', 'data_source': 'customer_data.csv', 'target_variable': 'churn', 'optimization_priority': 'accuracy'})
    print(f'Create result: {create_result}')
    if create_result['status'] == 'success':
        workflow_id = create_result['workflow_id']
        print(f'\nExecuting workflow {workflow_id}...')
        execute_result = workflow_gen.execute({'action': 'execute', 'workflow_id': workflow_id})
        print(f'Execute result: {execute_result}')
    print('\nAvailable templates:')
    templates_result = workflow_gen.execute({'action': 'templates'})
    print(f'Templates: {templates_result}')