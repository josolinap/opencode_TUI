from functools import lru_cache
'\nCheck current model usage and test model switching in Neo-Clone\n'
from llm_client_opencode import OpencodeLLMClient
from config_opencode import load_config, get_current_opencode_model
import logging

@lru_cache(maxsize=128)
def check_current_model_usage():
    logging.basicConfig(level=logging.INFO)
    print('Neo-Clone Model Usage Analysis')
    print('=' * 50)
    print('\n1. Current Configuration:')
    print('-' * 30)
    current_opencode_model = get_current_opencode_model()
    print(f'Opencode Config Model: {current_opencode_model}')
    config = load_config()
    print(f'Config opencode_model: {config.opencode_model}')
    print(f'Config provider: {config.provider}')
    print(f'Config model_name: {config.model_name}')
    print('\n2. LLM Client Status:')
    print('-' * 30)
    client = OpencodeLLMClient(config)
    current_client_model = client.get_current_model()
    print(f'Client current model: {current_client_model}')
    available_models = client.get_available_models()
    print(f'Total available models: {len(available_models)}')
    print('\n3. Top Working Models:')
    print('-' * 30)
    for (i, model) in enumerate(available_models[:10]):
        status = '[CURRENT]' if model == current_client_model else '        '
        print(f'{status} {i + 1:2d}. {model}')
    print('\n4. Model Switching Test:')
    print('-' * 30)
    try:
        print('Testing switch to opencode/grok-code...')
        client.set_model('opencode/grok-code')
        switched_model = client.get_current_model()
        print(f'[OK] Successfully switched to: {switched_model}')
        messages = [{'role': 'user', 'content': 'What is 3+3?'}]
        response = client.chat(messages, timeout=15)
        if response.content and (not response.content.startswith('[')):
            print(f'[OK] Query successful: {response.content}')
            print(f'   Response time: {response.response_time:.2f}s')
        else:
            print(f'[FAIL] Query failed: {response.content}')
    except Exception as e:
        print(f'[ERROR] Switch failed: {e}')
    try:
        print(f'\nSwitching back to {current_opencode_model}...')
        client.set_model(current_opencode_model)
        restored_model = client.get_current_model()
        print(f'[OK] Restored to: {restored_model}')
    except Exception as e:
        print(f'[ERROR] Restore failed: {e}')
    print('\n5. Other Free Models Status:')
    print('-' * 30)
    free_model_categories = {'Opencode': [m for m in available_models if m.startswith('opencode/')], 'Gemini': [m for m in available_models if m.startswith('gemini/')], 'Ollama': [m for m in available_models if m.startswith('ollama/')], 'HuggingFace': [m for m in available_models if m.startswith('huggingface/')]}
    for (category, models) in free_model_categories.items():
        print(f'{category}: {len(models)} models')
        for model in models[:3]:
            print(f'   - {model}')
        if len(models) > 3:
            print(f'   ... and {len(models) - 3} more')
    print('\n6. Usage Summary:')
    print('-' * 30)
    print(f'[OK] Neo-Clone is currently using: {current_client_model}')
    print(f'[OK] Can switch to {len(available_models)} models')
    print(f"[OK] {len(free_model_categories['Opencode'])} Opencode models available")
    print(f"[OK] {len(free_model_categories['Gemini'])} Gemini models available")
    print(f"[OK] {len(free_model_categories['Ollama'])} Ollama models available")
    print(f"[OK] {len(free_model_categories['HuggingFace'])} HuggingFace models available")
if __name__ == '__main__':
    check_current_model_usage()