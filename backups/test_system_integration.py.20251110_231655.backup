from functools import lru_cache
'\ntest_system_integration.py - Simple Integration Test for Neo-Clone Advanced Features\n'
import sys
import os
import time
import json
from pathlib import Path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'skills'))

@lru_cache(maxsize=128)
def test_basic_functionality():
    """Test basic functionality of all systems"""
    print('=' * 60)
    print('NEO-CLONE SYSTEM INTEGRATION TEST')
    print('=' * 60)
    results = {}
    print('\n1. Testing Smart Routing System...')
    try:
        from skills.smart_routing import SmartRouter
        router = SmartRouter()
        context = router.analyze_context(user_input='Write a Python function to sort data', conversation_history=[], user_preferences={})
        decision = router.route_request(context)
        print(f'   - Context analysis: {context.domain} domain, complexity: {context.complexity_score:.2f}')
        print(f'   - Selected model: {decision.selected_model}')
        print(f'   - Confidence: {decision.confidence_score:.2f}')
        print(f'   - Reasoning: {len(decision.reasoning)} factors considered')
        results['smart_routing'] = True
    except Exception as e:
        print(f'   - ERROR: {e}')
        results['smart_routing'] = False
    print('\n2. Testing Autonomous Intelligence...')
    try:
        from skills.autonomous_intelligence import AutonomousIntelligence
        intel = AutonomousIntelligence()
        intel.record_interaction(user_input='Test coding request', response="Here's a Python function...", model_used='opencode/grok-code', context={'domain': 'coding'}, performance_metrics={'response_time': 1.2, 'success': True})
        insights = intel.get_learning_insights()
        print(f'   - Interactions recorded: {len(intel.interaction_history)}')
        print(f"   - Learning patterns: {len(insights.get('learning_patterns', {}))}")
        print(f"   - Total interactions: {insights.get('total_interactions', 0)}")
        results['autonomous_intelligence'] = True
    except Exception as e:
        print(f'   - ERROR: {e}')
        results['autonomous_intelligence'] = False
    print('\n3. Testing Real-time Analytics...')
    try:
        from skills.realtime_analytics import RealTimeAnalytics
        analytics = RealTimeAnalytics()
        analytics.record_metric('response_time', 1.5, {'model': 'test'})
        analytics.record_metric('request', 1, {'endpoint': 'test'})
        dashboard = analytics.get_real_time_dashboard()
        print(f'   - Metrics recorded: {len(analytics.metrics_history)}')
        print(f"   - Dashboard available: {dashboard.get('status') != 'no_data'}")
        print(f"   - System health: {dashboard.get('system_health', 0):.2f}")
        results['realtime_analytics'] = True
    except Exception as e:
        print(f'   - ERROR: {e}')
        results['realtime_analytics'] = False
    print('\n4. Testing Automated Workflows...')
    try:
        from skills.automated_workflows import WorkflowEngine
        engine = WorkflowEngine()
        steps = [{'name': 'Test Step', 'action': 'log', 'parameters': {'message': 'Test workflow execution'}}]
        workflow_id = engine.create_workflow(name='Test Workflow', description='A test workflow for validation', steps=steps)
        execution_id = engine.execute_workflow(workflow_id)
        print(f'   - Workflow created: {workflow_id[:8]}...')
        print(f'   - Execution started: {execution_id[:8]}...')
        print(f'   - Total workflows: {len(engine.workflows)}')
        results['automated_workflows'] = True
    except Exception as e:
        print(f'   - ERROR: {e}')
        results['automated_workflows'] = False
    print('\n5. Testing ML Engineering...')
    try:
        from skills.advanced_ml_engineering import AdvancedMLEngineering
        ml_engineer = AdvancedMLEngineering()
        overview = ml_engineer.get_workspace_overview()
        print(f'   - Workspace initialized: {ml_engineer.workspace_path.exists()}')
        print(f"   - Experiments: {overview['experiments']['total']}")
        print(f"   - Models: {overview['models']['total']}")
        print(f"   - Pipelines: {overview['pipelines']['total']}")
        results['ml_engineering'] = True
    except Exception as e:
        print(f'   - ERROR: {e}')
        results['ml_engineering'] = False
    print('\n6. Testing Enhanced Integration...')
    try:
        from skills.enhanced_opencode_integration import EnhancedOpencodeIntegration
        integration = EnhancedOpencodeIntegration()
        result = integration.process_request(user_input='Help me debug this Python code', conversation_history=[], user_preferences={})
        print(f'   - Integration initialized: {integration.is_initialized}')
        print(f"   - Request processed: {result.get('success', True)}")
        print(f"   - Model used: {result.get('model_used', 'unknown')}")
        print(f"   - Response time: {result.get('response_time', 0):.2f}s")
        results['enhanced_integration'] = True
    except Exception as e:
        print(f'   - ERROR: {e}')
        results['enhanced_integration'] = False
    print('\n' + '=' * 60)
    print('SYSTEM INTEGRATION TEST RESULTS')
    print('=' * 60)
    total_tests = len(results)
    passed_tests = sum(results.values())
    for (system, status) in results.items():
        status_symbol = 'PASS' if status else 'FAIL'
        print(f"   {system.replace('_', ' ').title()}: {status_symbol}")
    print(f'\nOverall: {passed_tests}/{total_tests} systems working ({passed_tests / total_tests * 100:.1f}%)')
    if passed_tests == total_tests:
        print('Status: ALL SYSTEMS OPERATIONAL')
    elif passed_tests >= total_tests * 0.8:
        print('Status: MOSTLY OPERATIONAL')
    else:
        print('Status: NEEDS ATTENTION')
    return results

def test_model_integration():
    """Test model integration and switching"""
    print('\n' + '=' * 60)
    print('MODEL INTEGRATION TEST')
    print('=' * 60)
    try:
        config_path = Path('opencode.json')
        if config_path.exists():
            with open(config_path, 'r') as f:
                config = json.load(f)
            print(f"   - Configuration loaded: {config.get('model', 'unknown')}")
            print(f"   - Provider: {config.get('provider', 'unknown')}")
            model = config.get('model', '')
            if 'big-pickle' in model or 'grok-code' in model:
                print('   - Model status: CONFIGURED')
            else:
                print('   - Model status: NEEDS CONFIGURATION')
        else:
            print('   - Configuration: NOT FOUND')
    except Exception as e:
        print(f'   - Configuration error: {e}')

def test_performance():
    """Test system performance"""
    print('\n' + '=' * 60)
    print('PERFORMANCE TEST')
    print('=' * 60)
    try:
        from skills.smart_routing import SmartRouter
        router = SmartRouter()
        start_time = time.time()
        test_requests = ['Write a Python function', 'Analyze this data', 'Debug my code', 'Create a workflow', 'Train a model']
        for request in test_requests:
            context = router.analyze_context(request)
            decision = router.route_request(context)
        end_time = time.time()
        avg_time = (end_time - start_time) / len(test_requests)
        print(f'   - Requests processed: {len(test_requests)}')
        print(f'   - Total time: {end_time - start_time:.3f}s')
        print(f'   - Average per request: {avg_time:.3f}s')
        if avg_time < 0.1:
            print('   - Performance: EXCELLENT')
        elif avg_time < 0.5:
            print('   - Performance: GOOD')
        else:
            print('   - Performance: NEEDS OPTIMIZATION')
    except Exception as e:
        print(f'   - Performance test error: {e}')

def main():
    """Main test function"""
    print('NEO-CLONE ADVANCED SYSTEM TEST')
    print('Testing all advanced features and integration...')
    integration_results = test_basic_functionality()
    test_model_integration()
    test_performance()
    print('\n' + '=' * 60)
    print('FINAL REPORT')
    print('=' * 60)
    total_systems = len(integration_results)
    working_systems = sum(integration_results.values())
    print(f'Systems Tested: {total_systems}')
    print(f'Systems Working: {working_systems}')
    print(f'Success Rate: {working_systems / total_systems * 100:.1f}%')
    if working_systems == total_systems:
        print('\nSTATUS: ALL SYSTEMS READY FOR PRODUCTION')
    elif working_systems >= total_systems * 0.8:
        print('\nSTATUS: SYSTEMS MOSTLY READY - Minor issues to address')
    else:
        print('\nSTATUS: SYSTEMS NEED ATTENTION - Major issues to resolve')
    report = {'timestamp': time.time(), 'integration_results': integration_results, 'systems_working': working_systems, 'total_systems': total_systems, 'success_rate': working_systems / total_systems * 100}
    with open('system_test_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    print(f'\nDetailed report saved to: system_test_report.json')
if __name__ == '__main__':
    main()