from functools import lru_cache
"\nbrain_opencode.py - Opencode-compatible central reasoning engine for Neo-Clone\n\nThis module extends the original brain.py to work seamlessly with Opencode's\nmodel selection system while maintaining all existing functionality.\n\nFeatures:\n- Integration with Opencode model selection\n- Backward compatibility with original brain.py\n- Enhanced model management\n- Single LLM client per process pattern\n- Conversation context/history management\n- Intent parsing and skill routing\n- MiniMax Agent integration\n"
import logging
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from config_opencode import Config, load_config, get_current_opencode_model
from skills import SkillRegistry
from llm_client_opencode import LLMClient, LLMResponse
import time
logger = logging.getLogger(__name__)

@dataclass
class Message:
    role: str
    content: str
    timestamp: Optional[float] = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()

class ConversationHistory:

    def __init__(self, max_messages: int=20):
        self.max_messages = max_messages
        self._messages: List[Message] = []
        self.session_start = time.time()

    def add(self, role: str, content: str):
        self._messages.append(Message(role=role, content=content))
        if len(self._messages) > self.max_messages:
            self._messages = self._messages[-self.max_messages:]

    def to_list(self) -> List[Dict[str, str]]:
        return [{'role': m.role, 'content': m.content} for m in self._messages]

    def to_enhanced_list(self) -> List[Dict[str, Any]]:
        """Enhanced message list with metadata"""
        return [{'role': m.role, 'content': m.content, 'timestamp': m.timestamp, 'session_time': m.timestamp - self.session_start} for m in self._messages]

    def clear(self):
        self._messages = []
        self.session_start = time.time()

    def get_stats(self) -> Dict[str, Any]:
        """Get conversation statistics"""
        return {'message_count': len(self._messages), 'session_duration': time.time() - self.session_start, 'user_messages': len([m for m in self._messages if m.role == 'user']), 'assistant_messages': len([m for m in self._messages if m.role == 'assistant'])}

class OpencodeBrain:

    def __init__(self, config: Config, skills: SkillRegistry, llm_client: Optional[LLMClient]=None):
        self.cfg = config
        self.skills = skills
        self.llm = llm_client or LLMClient(config)
        self.history = ConversationHistory(max_messages=20)
        self.model_switch_count = 0
        self.opencode_model = get_current_opencode_model()
        if self.opencode_model:
            logger.info(f'OpencodeBrain initialized with Opencode model: {self.opencode_model}')
        else:
            logger.info('OpencodeBrain initialized with local configuration')

    @lru_cache(maxsize=128)
    def parse_intent(self, text: str) -> Dict[str, str]:
        """Enhanced intent parsing with MiniMax Agent integration"""
        lowered = text.lower().strip()
        if lowered.startswith('/model') or 'switch model' in lowered or 'change model' in lowered:
            return {'intent': 'model_switch', 'action': 'switch_model'}
        if any((word in lowered for word in ['train', 'model', 'simulate', 'recommend', 'ml'])):
            return {'intent': 'skill', 'skill': 'ml_training'}
        if any((word in lowered for word in ['sentiment', 'analyze', 'moderate', 'toxic', 'text'])):
            return {'intent': 'skill', 'skill': 'text_analysis'}
        if any((word in lowered for word in ['csv', 'json', 'data', 'summary', 'stats', 'inspect'])):
            return {'intent': 'skill', 'skill': 'data_inspector'}
        if any((word in lowered for word in ['code', 'python', 'generate', 'snippet', 'explain', 'programming'])):
            return {'intent': 'skill', 'skill': 'code_generation'}
        if any((word in lowered for word in ['file', 'manage', 'organize', 'directory'])):
            return {'intent': 'skill', 'skill': 'file_manager'}
        if any((word in lowered for word in ['search', 'web', 'internet', 'find'])):
            return {'intent': 'skill', 'skill': 'web_search'}
        if any((word in lowered for word in ['minimax', 'reasoning', 'analyze', 'generate', 'think'])):
            return {'intent': 'minimax', 'skill': 'minimax_agent'}
        return {'intent': 'chat'}

    def route_to_skill(self, skill_name: str, text: str, context: Optional[Dict]=None) -> Dict:
        """Enhanced skill routing with context support"""
        try:
            skill = self.skills.get(skill_name)
            params = {'text': text}
            if context:
                params.update(context)
            result = skill.execute(params)
            return {'chosen_skill': skill_name, 'meta': {'description': skill.description, 'example': skill.example_usage, 'parameters': getattr(skill, 'parameters', {})}, 'output': result, 'reasoning': f"Chose skill '{skill_name}' due to detected keywords and context analysis."}
        except Exception as e:
            logger.error(f'Skill routing failed for {skill_name}: {e}')
            return {'error': f'Skill routing failed: {e}'}

    def handle_model_switch(self, text: str) -> str:
        """Handle model switching requests"""
        if text.startswith('/model'):
            parts = text.split(None, 1)
            if len(parts) > 1:
                new_model = parts[1].strip()
                return self.switch_model(new_model)
            else:
                return self.show_available_models()
        else:
            return 'Model switching format: /model <provider/model>'

    def switch_model(self, model: str) -> str:
        """Switch to a different model"""
        try:
            self.llm.set_model(model)
            self.model_switch_count += 1
            self.cfg.opencode_model = model
            if '/' in model:
                (provider, model_name) = model.split('/', 1)
                self.cfg.provider = provider
                self.cfg.model_name = model_name
            logger.info(f'Model switched to: {model}')
            return f'âœ… Model switched to: {model}'
        except Exception as e:
            error_msg = f'âŒ Failed to switch model: {str(e)}'
            logger.error(error_msg)
            return error_msg

    def show_available_models(self) -> str:
        """Show available models"""
        models = self.llm.get_available_models()
        current = self.llm.get_current_model()
        if not models:
            return 'No models available. Please configure your LLM provider.'
        result = [f'ðŸ“‹ Available Models (current: {current}):\n']
        for model in models[:10]:
            marker = 'â–¶ï¸ ' if model == current else '  '
            result.append(f'{marker}{model}')
        if len(models) > 10:
            result.append(f'... and {len(models) - 10} more')
        result.append(f"\nðŸ’¡ Use '/model {(models[0] if models else 'provider/model')}' to switch")
        return '\n'.join(result)

    def send_message(self, text: str, context: Optional[Dict]=None) -> str:
        """Enhanced message processing with Opencode integration"""
        self.history.add('user', text)
        intent = self.parse_intent(text)
        if intent.get('intent') == 'model_switch':
            response = self.handle_model_switch(text)
            self.history.add('assistant', response)
            return response
        if intent.get('intent') == 'skill' and intent.get('skill'):
            skill_name = intent['skill']
            result = self.route_to_skill(skill_name, text, context)
            self.history.add('assistant', f'[Skill:{skill_name}] {result}')
            return f"ðŸ¤– [Neo Reasoning] {result['reasoning']}\n\nðŸ“¤ [Skill Output]\n{result['output']}"
        if intent.get('intent') == 'minimax' and intent.get('skill') == 'minimax_agent':
            try:
                minimax_skill = self.skills.get('minimax_agent')
                result = minimax_skill.execute({'text': text, 'context': context})
                self.history.add('assistant', f'[MiniMax Agent] {result}')
                return f'ðŸ§  [MiniMax Agent] {result}'
            except Exception as e:
                error_msg = f'âŒ MiniMax Agent error: {str(e)}'
                logger.error(error_msg)
                return error_msg
        try:
            response = self.llm.chat(self.history.to_list())
            self.history.add('assistant', response.content)
            model_info = f' ({response.provider}/{response.model})' if response.provider != 'error' else ''
            return response.content + model_info
        except Exception as e:
            error_msg = f'[Neo Error] LLM request failed: {str(e)}'
            logger.error(error_msg)
            return error_msg

    def get_status(self) -> Dict[str, Any]:
        """Get brain status including Opencode integration info"""
        current_model = self.llm.get_current_model()
        status = {'current_model': current_model, 'provider': self.cfg.provider, 'model_name': self.cfg.model_name, 'opencode_model': self.opencode_model, 'is_opencode_available': self.opencode_model is not None, 'model_switches': self.model_switch_count, 'conversation_stats': self.history.get_stats(), 'available_skills': list(self.skills.skills.keys()), 'config_source': 'opencode' if self.opencode_model else 'local'}
        return status

    def clear_history(self):
        """Clear conversation history"""
        self.history.clear()

class Brain(OpencodeBrain):
    """Backward compatible Brain class"""
    pass