from functools import lru_cache
'\nShow all available models in Neo-Clone\n'
from llm_client_opencode import OpencodeLLMClient
from config_opencode import load_config

@lru_cache(maxsize=128)
def show_available_models():
    config = load_config()
    client = OpencodeLLMClient(config)
    models = client.get_available_models()
    print('Neo-Clone Available Models:')
    print('=' * 60)
    print(f'Total: {len(models)} models')
    print()
    print('PRIORITY MODELS (Top 15):')
    print('-' * 40)
    for (i, model) in enumerate(models[:15], 1):
        if model == 'opencode/big-pickle':
            status = '[VERIFIED WORKING]'
        elif model.startswith('opencode/'):
            status = '[Opencode Free]'
        elif model.startswith('gemini/'):
            status = '[Gemini CLI Free]'
        elif model.startswith('ollama/'):
            status = '[Local Free]'
        else:
            status = '[Available]'
        print(f'{i:2d}. {model:<35} {status}')
    print()
    print('MODEL CATEGORIES:')
    print('-' * 40)
    opencode_count = len([m for m in models if m.startswith('opencode/')])
    gemini_count = len([m for m in models if m.startswith('gemini/')])
    ollama_count = len([m for m in models if m.startswith('ollama/')])
    other_count = len(models) - opencode_count - gemini_count - ollama_count
    print(f'Opencode Models: {opencode_count} (1 verified working)')
    print(f'Gemini CLI Models: {gemini_count} (free via CLI)')
    print(f'Ollama Models: {ollama_count} (local, free)')
    print(f'Other Models: {other_count} (various sources)')
    print()
    print('CURRENTLY WORKING:')
    print('-' * 40)
    print('opencode/big-pickle - VERIFIED WORKING (cost: 0)')
    print()
    print('USAGE EXAMPLE:')
    print('-' * 40)
    print('client.set_model("opencode/big-pickle")')
    print('response = client.chat(messages)')
if __name__ == '__main__':
    show_available_models()