from functools import lru_cache
'\ndirect_model_tester.py - Direct testing of Opencode models\n\nTests models directly with Opencode CLI to find more working ones,\nthen updates the system to use them.\n'
import subprocess
import json
import time
from typing import List, Dict, Any, Tuple

@lru_cache(maxsize=128)
def get_all_opencode_models() -> List[str]:
    """Get all models directly from Opencode CLI"""
    try:
        result = subprocess.run(['opencode', 'models'], capture_output=True, text=True, timeout=30)
        if result.returncode == 0:
            models = []
            for line in result.stdout.strip().split('\n'):
                model = line.strip()
                if model and '/' in model:
                    models.append(model)
            return models
        else:
            return []
    except:
        return []

def test_model_directly(model: str) -> Dict[str, Any]:
    """Test model directly with Opencode CLI"""
    try:
        commands = [['opencode', 'run', '--model', model, '--format', 'json'], ['opencode', 'run', '--model', model], ['opencode', 'run', model]]
        for cmd in commands:
            try:
                start_time = time.time()
                result = subprocess.run(cmd, input='What is 2+2? Answer with just the number.', capture_output=True, text=True, timeout=30)
                response_time = time.time() - start_time
                if result.returncode == 0:
                    response = result.stdout.strip()
                    if response and len(response) > 0 and (not response.startswith('[')):
                        return {'works': True, 'response': response, 'response_time': response_time, 'command_used': cmd}
            except subprocess.TimeoutExpired:
                continue
            except:
                continue
        return {'works': False, 'error': 'Command failed'}
    except Exception as e:
        return {'works': False, 'error': str(e)}

def analyze_opencode_system() -> Dict[str, Any]:
    """Analyze Opencode system for improvements"""
    print('DIRECT OPENCODE MODEL ANALYSIS')
    print('=' * 50)
    all_models = get_all_opencode_models()
    print(f'Found {len(all_models)} models in Opencode CLI')
    working_models = []
    failed_models = []
    for (i, model) in enumerate(all_models, 1):
        print(f'\n[{i}/{len(all_models)}] Testing: {model}')
        result = test_model_directly(model)
        if result['works']:
            working_models.append({'name': model, 'response': result['response'], 'response_time': result['response_time'], 'command': result['command_used']})
            print(f"   WORKING - {result['response_time']:.2f}s")
            print(f"   Response: {result['response'][:50]}...")
        else:
            failed_models.append({'name': model, 'error': result['error']})
            print(f"   FAILED - {result['error'][:50]}...")
    working_patterns = analyze_working_patterns(working_models)
    system_status = check_opencode_system()
    return {'total_models': len(all_models), 'working_models': working_models, 'failed_models': failed_models, 'working_count': len(working_models), 'failed_count': len(failed_models), 'success_rate': len(working_models) / len(all_models) * 100, 'working_patterns': working_patterns, 'system_status': system_status}

def analyze_working_patterns(working_models: List[Dict]) -> Dict[str, Any]:
    """Analyze patterns in working models"""
    patterns = {'providers': {}, 'model_types': {}, 'command_formats': {}}
    for model in working_models:
        name = model['name']
        if '/' in name:
            provider = name.split('/')[0]
            patterns['providers'][provider] = patterns['providers'].get(provider, 0) + 1
        if any((keyword in name.lower() for keyword in ['gpt', 'claude', 'gemini'])):
            model_type = 'llm'
        elif any((keyword in name.lower() for keyword in ['embedding'])):
            model_type = 'embedding'
        else:
            model_type = 'other'
        patterns['model_types'][model_type] = patterns['model_types'].get(model_type, 0) + 1
        cmd = str(model['command'])
        if '--format json' in cmd:
            cmd_format = 'json_format'
        elif '--model' in cmd:
            cmd_format = 'model_flag'
        else:
            cmd_format = 'direct'
        patterns['command_formats'][cmd_format] = patterns['command_formats'].get(cmd_format, 0) + 1
    return patterns

def check_opencode_system() -> Dict[str, Any]:
    """Check Opencode system status"""
    status = {}
    try:
        result = subprocess.run(['opencode', '--version'], capture_output=True, text=True, timeout=10)
        status['version'] = result.stdout.strip() if result.returncode == 0 else 'Unknown'
    except:
        status['version'] = 'Error getting version'
    try:
        result = subprocess.run(['opencode', 'auth', '--list'], capture_output=True, text=True, timeout=10)
        status['auth_configured'] = result.returncode == 0
        status['auth_details'] = result.stdout.strip() if result.returncode == 0 else 'Not configured'
    except:
        status['auth_configured'] = False
        status['auth_details'] = 'Error checking auth'
    try:
        result = subprocess.run(['opencode', 'upgrade', '--check'], capture_output=True, text=True, timeout=30)
        status['upgrade_available'] = result.returncode != 0
        status['upgrade_details'] = result.stdout.strip() if result.returncode != 0 else 'Up to date'
    except:
        status['upgrade_available'] = False
        status['upgrade_details'] = 'Error checking upgrades'
    return status

def update_llm_client_with_working_models(working_models: List[Dict]):
    """Update LLM client to include newly discovered working models"""
    print(f'\nUPDATING LLM CLIENT WITH {len(working_models)} WORKING MODELS')
    print('-' * 60)
    working_model_names = [model['name'] for model in working_models]
    with open('discovered_working_models.py', 'w') as f:
        f.write('"""Discovered working models from direct testing"""')
        f.write('\n\nDISCOVERED_WORKING_MODELS = ')
        f.write(json.dumps(working_model_names, indent=4))
        f.write('\n')
    patch_content = f'"""\nllm_client_enhancement.py - Enhanced LLM client with discovered models\n\nThis patch adds the newly discovered working models to the LLM client.\n"""\n\n# Discovered working models from direct testing\nDISCOVERED_WORKING_MODELS = {json.dumps(working_model_names, indent=4)}\n\ndef enhance_llm_client():\n    """Enhance LLM client with discovered models"""\n    print(f"Enhancing LLM client with {{len(DISCOVERED_WORKING_MODELS)}} discovered models")\n    return DISCOVERED_WORKING_MODELS\n'
    with open('llm_client_enhancement.py', 'w') as f:
        f.write(patch_content)
    print(f'Updated working models list with {len(working_model_names)} models')
    print(f'Files created:')
    print(f'  • discovered_working_models.py')
    print(f'  • llm_client_enhancement.py')

def main():
    results = analyze_opencode_system()
    print('\n' + '=' * 60)
    print('DIRECT MODEL ANALYSIS RESULTS')
    print('=' * 60)
    print(f"Total models tested: {results['total_models']}")
    print(f"Working models: {results['working_count']}")
    print(f"Failed models: {results['failed_count']}")
    print(f"Success rate: {results['success_rate']:.1f}%")
    if results['working_models']:
        print(f'\nWORKING MODELS DISCOVERED:')
        print('-' * 40)
        for (i, model) in enumerate(results['working_models'], 1):
            print(f"{i}. {model['name']}")
            print(f"   Response time: {model['response_time']:.2f}s")
            print(f"   Response: {model['response'][:50]}...")
            print(f"   Command: {' '.join(model['command'])}")
            print('')
    print(f'\nWORKING PATTERNS:')
    print('-' * 30)
    patterns = results['working_patterns']
    print(f"Providers: {patterns['providers']}")
    print(f"Model types: {patterns['model_types']}")
    print(f"Command formats: {patterns['command_formats']}")
    print(f'\nSYSTEM STATUS:')
    print('-' * 30)
    status = results['system_status']
    print(f"Version: {status['version']}")
    print(f"Auth configured: {status['auth_configured']}")
    print(f"Upgrade available: {status['upgrade_available']}")
    if results['working_models']:
        update_llm_client_with_working_models(results['working_models'])
    print(f'\nDIRECT MODEL ANALYSIS COMPLETE!')
if __name__ == '__main__':
    main()