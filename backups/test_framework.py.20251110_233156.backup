from functools import lru_cache
'\nNeo-Clone Testing Framework\nComprehensive testing infrastructure for skills and system components\n'
import unittest
import sys
import os
import time
import json
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime
import threading
import queue
import tempfile
import shutil
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class TestResult:
    """Enhanced test result with detailed metrics"""

    def __init__(self, test_name: str, test_class: str):
        self.test_name = test_name
        self.test_class = test_class
        self.start_time = None
        self.end_time = None
        self.duration = 0.0
        self.status = 'pending'
        self.error_message = None
        self.exception = None
        self.assertions = []
        self.metrics = {}

    def start(self):
        self.start_time = time.time()
        self.status = 'running'

    def end(self, status: str, error_message: str=None, exception: Exception=None):
        self.end_time = time.time()
        self.duration = self.end_time - self.start_time
        self.status = status
        self.error_message = error_message
        self.exception = exception

    def add_assertion(self, assertion_type: str, expected: Any, actual: Any, passed: bool):
        self.assertions.append({'type': assertion_type, 'expected': expected, 'actual': actual, 'passed': passed, 'timestamp': time.time()})

    def add_metric(self, name: str, value: Any):
        self.metrics[name] = value

    def to_dict(self) -> Dict[str, Any]:
        return {'test_name': self.test_name, 'test_class': self.test_class, 'status': self.status, 'duration': self.duration, 'error_message': self.error_message, 'assertions': self.assertions, 'metrics': self.metrics, 'timestamp': datetime.now().isoformat()}

class SkillTestCase(unittest.TestCase):
    """Base test case for skill testing with enhanced capabilities"""

    def setUp(self):
        """Set up test environment"""
        self.test_results = []
        self.temp_dir = tempfile.mkdtemp()
        self.start_time = time.time()

    def tearDown(self):
        """Clean up test environment"""
        shutil.rmtree(self.temp_dir, ignore_errors=True)
        duration = time.time() - self.start_time
        print('.2f')

    def assertSkillResponse(self, response: Dict[str, Any], expected_keys: List[str]=None, success_required: bool=True):
        """Assert that skill response has expected structure"""
        if expected_keys is None:
            expected_keys = ['success']
        for key in expected_keys:
            self.assertIn(key, response, f'Response missing required key: {key}')
        if success_required:
            self.assertTrue(response.get('success', False), f"Skill execution failed: {response.get('error', 'Unknown error')}")

    def assertPerformance(self, operation: Callable, max_duration: float=1.0):
        """Assert that operation completes within time limit"""
        start_time = time.time()
        result = operation()
        duration = time.time() - start_time
        self.assertLess(duration, max_duration, '.2f')
        return result

    def create_temp_file(self, content: str, filename: str=None) -> str:
        """Create a temporary file with content"""
        if filename is None:
            filename = f'test_{int(time.time())}.txt'
        filepath = os.path.join(self.temp_dir, filename)
        with open(filepath, 'w') as f:
            f.write(content)
        return filepath

    def load_skill(self, skill_name: str):
        """Load a skill dynamically"""
        try:
            module = __import__(f'skills.{skill_name}', fromlist=[skill_name])

            # Try different class name patterns
            possible_names = [
                f"{skill_name.title().replace('_', '')}Skill",  # Standard pattern
                f"{skill_name.title().replace('_', '')}",       # Direct title case
                skill_name.replace('_', ' ').title().replace(' ', ''),  # Alternative
            ]

            skill_class = None
            for name in possible_names:
                try:
                    skill_class = getattr(module, name)
                    break
                except AttributeError:
                    continue

            if skill_class is None:
                # Try to find any class that ends with 'Skill' or has 'skill' in name
                for attr_name in dir(module):
                    if attr_name.endswith('Skill') or 'skill' in attr_name.lower():
                        skill_class = getattr(module, attr_name)
                        break

            if skill_class is None:
                # Try to find the main class (often the module name in title case)
                main_class_name = skill_name.replace('_', ' ').title().replace(' ', '')
                try:
                    skill_class = getattr(module, main_class_name)
                except AttributeError:
                    # Try with common abbreviations in uppercase
                    alt_name = main_class_name.replace('Ml', 'ML').replace('Ai', 'AI').replace('Api', 'API')
                    try:
                        skill_class = getattr(module, alt_name)
                    except AttributeError:
                        pass

            if skill_class is None:
                raise AttributeError(f"No suitable skill class found in {skill_name}")

            return skill_class()
        except Exception as e:
            self.fail(f'Failed to load skill {skill_name}: {e}')

class MockSkill:
    """Mock skill for testing dependencies"""

    def __init__(self, name: str, responses: Dict[str, Any]=None):
        self.name = name
        self.responses = responses or {}
        self.call_history = []

    def execute(self, params: Dict[str, Any]) -> Dict[str, Any]:
        self.call_history.append(params)
        return self.responses.get(params.get('action', 'default'), {'success': True})

class TestRunner:
    """Advanced test runner with parallel execution and reporting"""

    def __init__(self):
        self.test_results = []
        self.start_time = None
        self.end_time = None

    def run_tests(self, test_modules: List[str], parallel: bool=False) -> Dict[str, Any]:
        """Run tests with comprehensive reporting"""
        self.start_time = time.time()
        if parallel:
            return self._run_parallel(test_modules)
        else:
            return self._run_sequential(test_modules)

    def _run_sequential(self, test_modules: List[str]) -> Dict[str, Any]:
        """Run tests sequentially"""
        results = []
        for module_name in test_modules:
            try:
                module = __import__(f'tests.{module_name}', fromlist=[module_name])
                test_classes = [obj for obj in module.__dict__.values() if isinstance(obj, type) and issubclass(obj, unittest.TestCase)]
                for test_class in test_classes:
                    suite = unittest.TestLoader().loadTestsFromTestCase(test_class)
                    runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
                    result = runner.run(suite)
                    results.append({'module': module_name, 'class': test_class.__name__, 'tests_run': result.testsRun, 'failures': len(result.failures), 'errors': len(result.errors), 'skipped': len(result.skipped)})
            except Exception as e:
                results.append({'module': module_name, 'error': str(e), 'tests_run': 0, 'failures': 0, 'errors': 1, 'skipped': 0})
        self.end_time = time.time()
        return self._generate_report(results)

    def _run_parallel(self, test_modules: List[str]) -> Dict[str, Any]:
        """Run tests in parallel using threading"""
        results = []
        result_queue = queue.Queue()

        def run_module(module_name: str):
            result = self._run_sequential([module_name])
            result_queue.put(result)
        threads = []
        for module_name in test_modules:
            thread = threading.Thread(target=run_module, args=(module_name,))
            thread.start()
            threads.append(thread)
        for thread in threads:
            thread.join()
            if not result_queue.empty():
                results.append(result_queue.get())
        self.end_time = time.time()
        return self._aggregate_parallel_results(results)

    def _generate_report(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comprehensive test report"""
        total_tests = sum((r.get('tests_run', 0) for r in results))
        total_failures = sum((r.get('failures', 0) for r in results))
        total_errors = sum((r.get('errors', 0) for r in results))
        total_skipped = sum((r.get('skipped', 0) for r in results))
        duration = self.end_time - self.start_time
        return {'summary': {'total_tests': total_tests, 'passed': total_tests - total_failures - total_errors, 'failed': total_failures, 'errors': total_errors, 'skipped': total_skipped, 'duration': duration, 'success_rate': (total_tests - total_failures - total_errors) / total_tests if total_tests > 0 else 0}, 'results': results, 'timestamp': datetime.now().isoformat(), 'environment': {'python_version': sys.version, 'platform': sys.platform, 'working_directory': os.getcwd()}}

    def _aggregate_parallel_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Aggregate results from parallel test execution"""
        if not results:
            return self._generate_report([])
        combined = {'summary': {'total_tests': sum((r['summary']['total_tests'] for r in results)), 'passed': sum((r['summary']['passed'] for r in results)), 'failed': sum((r['summary']['failed'] for r in results)), 'errors': sum((r['summary']['errors'] for r in results)), 'skipped': sum((r['summary']['skipped'] for r in results)), 'duration': max((r['summary']['duration'] for r in results)), 'success_rate': 0.0}, 'results': [r for result in results for r in result.get('results', [])], 'timestamp': datetime.now().isoformat(), 'parallel_execution': True}
        total_tests = combined['summary']['total_tests']
        if total_tests > 0:
            passed = combined['summary']['passed']
            combined['summary']['success_rate'] = passed / total_tests
        return combined

    def save_report(self, report: Dict[str, Any], filename: str=None):
        """Save test report to file"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'test_report_{timestamp}.json'
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        print(f'Test report saved to: {filename}')

class SkillTester:
    """Specialized tester for skill validation"""

    def __init__(self):
        self.test_runner = TestRunner()

    @lru_cache(maxsize=128)
    def test_skill(self, skill_name: str) -> Dict[str, Any]:
        """Test a specific skill comprehensively"""
        print(f'Testing skill: {skill_name}')
        try:
            skill_module = __import__(f'skills.{skill_name}', fromlist=[skill_name])
            skill_class_name = f"{skill_name.title().replace('_', '')}Skill"
            skill_class = getattr(skill_module, skill_class_name)
            skill = skill_class()
        except Exception as e:
            return {'skill': skill_name, 'status': 'error', 'error': f'Failed to load skill: {e}'}
        results = {'skill': skill_name, 'status': 'testing', 'tests': []}
        try:
            response = skill.execute({})
            results['tests'].append({'name': 'basic_execution', 'status': 'passed' if response.get('success') else 'failed', 'response': response})
        except Exception as e:
            results['tests'].append({'name': 'basic_execution', 'status': 'error', 'error': str(e)})
        try:
            response = skill.execute({'invalid_param': 'test'})
            results['tests'].append({'name': 'parameter_validation', 'status': 'passed', 'response': response})
        except Exception as e:
            results['tests'].append({'name': 'parameter_validation', 'status': 'error', 'error': str(e)})
        try:
            import time
            start_time = time.time()
            for _ in range(10):
                skill.execute({})
            duration = time.time() - start_time
            results['tests'].append({'name': 'performance', 'status': 'passed' if duration < 5.0 else 'slow', 'duration': duration})
        except Exception as e:
            results['tests'].append({'name': 'performance', 'status': 'error', 'error': str(e)})
        passed_tests = sum((1 for test in results['tests'] if test['status'] == 'passed'))
        total_tests = len(results['tests'])
        results['status'] = 'passed' if passed_tests == total_tests else 'failed'
        results['success_rate'] = passed_tests / total_tests if total_tests > 0 else 0
        return results

    def test_all_skills(self) -> Dict[str, Any]:
        """Test all available skills"""
        skills_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'skills')
        skill_files = [f[:-3] for f in os.listdir(skills_dir) if f.endswith('.py') and (not f.startswith('__'))]
        results = []
        for skill_name in skill_files:
            result = self.test_skill(skill_name)
            results.append(result)
            print(f"âœ“ Tested {skill_name}: {result['status']} ({result.get('success_rate', 0):.1%})")
        total_skills = len(results)
        passed_skills = sum((1 for r in results if r['status'] == 'passed'))
        failed_skills = total_skills - passed_skills
        return {'summary': {'total_skills': total_skills, 'passed': passed_skills, 'failed': failed_skills, 'success_rate': passed_skills / total_skills if total_skills > 0 else 0}, 'results': results, 'timestamp': datetime.now().isoformat()}

def run_comprehensive_tests():
    """Run comprehensive test suite"""
    print('ðŸ§ª Running Neo-Clone Comprehensive Test Suite')
    print('=' * 60)
    runner = TestRunner()
    skill_tester = SkillTester()
    print('\nðŸ“Š Testing All Skills...')
    skill_results = skill_tester.test_all_skills()
    print('\nðŸ§ª Running Unit Tests...')
    test_modules = ['test_framework', 'test_tui_components']
    unit_results = runner.run_tests(test_modules, parallel=True)
    comprehensive_report = {'test_suite': 'Neo-Clone Comprehensive Test Suite', 'timestamp': datetime.now().isoformat(), 'skill_tests': skill_results, 'unit_tests': unit_results, 'overall_status': 'passed' if skill_results['summary']['success_rate'] >= 0.8 else 'failed', 'recommendations': []}
    if skill_results['summary']['success_rate'] < 0.8:
        comprehensive_report['recommendations'].append('Improve skill test coverage - some skills are failing')
    if unit_results['summary']['success_rate'] < 0.9:
        comprehensive_report['recommendations'].append('Add more unit tests for better code coverage')
    runner.save_report(comprehensive_report, 'comprehensive_test_report.json')
    print('\n' + '=' * 60)
    print('ðŸ“‹ TEST SUMMARY')
    print('=' * 60)
    print(f"Skills Tested: {skill_results['summary']['total_skills']}")
    print(f"Skills Passed: {skill_results['summary']['passed']}")
    print(f"Skills Failed: {skill_results['summary']['failed']}")
    print('.1%')
    print(f"Unit Tests: {unit_results['summary']['total_tests']}")
    print(f"Unit Passed: {unit_results['summary']['passed']}")
    print('.1%')
    print(f"Overall Status: {comprehensive_report['overall_status'].upper()}")
    if comprehensive_report['recommendations']:
        print('\nðŸ’¡ RECOMMENDATIONS:')
        for rec in comprehensive_report['recommendations']:
            print(f'  â€¢ {rec}')
    return comprehensive_report
if __name__ == '__main__':
    run_comprehensive_tests()