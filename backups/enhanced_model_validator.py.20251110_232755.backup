from functools import lru_cache
'\nEnhanced Model Validator with Auto-Switching Capabilities\n\nAdvanced model validation system with intelligent auto-switching,\nperformance monitoring, and predictive health assessment.\n'
import json
import time
import logging
import threading
import statistics
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import asyncio
import concurrent.futures
logger = logging.getLogger(__name__)

@dataclass
class ModelMetrics:
    """Performance metrics for a model"""
    name: str
    response_times: List[float]
    success_rate: float
    error_count: int
    total_requests: int
    last_success: Optional[float]
    last_failure: Optional[float]
    quality_score: float
    cost_per_request: float
    capabilities: List[str]
    health_score: float

    def __post_init__(self):
        if not self.response_times:
            self.response_times = []
        if self.last_success is None:
            self.last_success = time.time()
        if self.last_failure is None:
            self.last_failure = 0

@dataclass
class ValidationRequest:
    """Request for model validation"""
    model_name: str
    test_prompt: str
    timeout: float
    expected_response_type: str = 'text'

class EnhancedModelValidator:
    """Advanced model validation and auto-switching system"""

    def __init__(self, config_path: str='working_models.json'):
        self.config_path = Path(config_path)
        self.metrics: Dict[str, ModelMetrics] = {}
        self.validation_history: List[Dict] = []
        self.auto_switch_enabled = True
        self.health_check_interval = 300
        self.performance_window = 100
        self.min_success_rate = 0.7
        self.max_response_time = 10.0
        self._lock = threading.RLock()
        self._monitoring = False
        self._monitor_thread = None
        self._load_metrics()
        self._load_working_models()

    def _load_metrics(self):
        """Load historical metrics from file"""
        metrics_file = self.config_path.parent / 'model_metrics.json'
        try:
            if metrics_file.exists():
                with open(metrics_file, 'r') as f:
                    data = json.load(f)
                    for (model_name, model_data) in data.items():
                        self.metrics[model_name] = ModelMetrics(**model_data)
                logger.info(f'Loaded metrics for {len(self.metrics)} models')
        except Exception as e:
            logger.warning(f'Could not load metrics: {e}')

    def _save_metrics(self):
        """Save metrics to file"""
        metrics_file = self.config_path.parent / 'model_metrics.json'
        try:
            with open(metrics_file, 'w') as f:
                data = {name: asdict(metrics) for (name, metrics) in self.metrics.items()}
                json.dump(data, f, indent=2, default=str)
        except Exception as e:
            logger.warning(f'Could not save metrics: {e}')

    @lru_cache(maxsize=128)
    def _load_working_models(self):
        """Load current working models"""
        try:
            if self.config_path.exists():
                with open(self.config_path, 'r') as f:
                    data = json.load(f)
                    for model_result in data.get('results', []):
                        model_name = model_result['name']
                        if model_name not in self.metrics:
                            self.metrics[model_name] = ModelMetrics(name=model_name, response_times=[], success_rate=1.0 if model_result.get('working') else 0.0, error_count=0 if model_result.get('working') else 1, total_requests=1, last_success=time.time() if model_result.get('working') else None, last_failure=0 if model_result.get('working') else time.time(), quality_score=model_result.get('quality', 0.5), cost_per_request=0.0, capabilities=[], health_score=1.0 if model_result.get('working') else 0.0)
        except Exception as e:
            logger.warning(f'Could not load working models: {e}')

    def validate_model(self, model_name: str, test_prompt: str='What is 2+2?', timeout: float=10.0) -> Tuple[bool, float, str]:
        """Validate a single model with comprehensive testing"""
        start_time = time.time()
        try:
            from llm_client_opencode import LLMClient
            client = LLMClient()
            client.set_model(model_name)
            response = client.generate(test_prompt, timeout=timeout)
            response_time = time.time() - start_time
            if response and response.content:
                success = True
                error_msg = ''
                quality_score = self._assess_response_quality(response.content, test_prompt)
            else:
                success = False
                error_msg = 'No response received'
                quality_score = 0.0
                response_time = timeout
        except Exception as e:
            success = False
            response_time = time.time() - start_time
            error_msg = str(e)
            quality_score = 0.0
        self._update_model_metrics(model_name, success, response_time, quality_score)
        return (success, response_time, error_msg)

    def _assess_response_quality(self, response: str, prompt: str) -> float:
        """Assess the quality of a model response"""
        if not response:
            return 0.0
        quality = 0.5
        if 10 <= len(response) <= 1000:
            quality += 0.1
        if '2+2' in prompt or 'math' in prompt.lower():
            if any((num in response for num in ['4', 'four'])):
                quality += 0.3
        if not any((err in response.lower() for err in ['error', 'sorry', 'cannot', 'unable'])):
            quality += 0.1
        return min(1.0, quality)

    def _update_model_metrics(self, model_name: str, success: bool, response_time: float, quality_score: float):
        """Update metrics for a model"""
        with self._lock:
            if model_name not in self.metrics:
                self.metrics[model_name] = ModelMetrics(name=model_name, response_times=[], success_rate=0.0, error_count=0, total_requests=0, last_success=None, last_failure=None, quality_score=0.0, cost_per_request=0.0, capabilities=[], health_score=0.0)
            metrics = self.metrics[model_name]
            metrics.total_requests += 1
            if success:
                metrics.response_times.append(response_time)
                if len(metrics.response_times) > self.performance_window:
                    metrics.response_times = metrics.response_times[-self.performance_window:]
                metrics.last_success = time.time()
            else:
                metrics.error_count += 1
                metrics.last_failure = time.time()
            metrics.success_rate = (metrics.total_requests - metrics.error_count) / metrics.total_requests
            if metrics.quality_score == 0:
                metrics.quality_score = quality_score
            else:
                metrics.quality_score = 0.8 * metrics.quality_score + 0.2 * quality_score
            metrics.health_score = self._calculate_health_score(metrics)
            if metrics.total_requests % 10 == 0:
                self._save_metrics()

    def _calculate_health_score(self, metrics: ModelMetrics) -> float:
        """Calculate overall health score for a model"""
        score = 0.0
        score += metrics.success_rate * 0.4
        if metrics.response_times:
            avg_response_time = statistics.mean(metrics.response_times)
            response_score = max(0, 1 - avg_response_time / self.max_response_time)
            score += response_score * 0.3
        score += metrics.quality_score * 0.2
        if metrics.last_success:
            time_since_success = time.time() - metrics.last_success
            recency_score = max(0, 1 - time_since_success / 3600)
            score += recency_score * 0.1
        return min(1.0, score)

    def get_best_model(self, task_type: str='general', exclude_models: List[str]=None) -> Optional[str]:
        """Get the best model for a given task type"""
        with self._lock:
            if not self.metrics:
                return None
            exclude_models = exclude_models or []
            candidates = []
            for (model_name, metrics) in self.metrics.items():
                if model_name in exclude_models:
                    continue
                if metrics.success_rate < self.min_success_rate or metrics.health_score < 0.5:
                    continue
                task_score = self._calculate_task_score(metrics, task_type)
                candidates.append((model_name, task_score))
            if not candidates:
                return None
            candidates.sort(key=lambda x: x[1], reverse=True)
            return candidates[0][0]

    def _calculate_task_score(self, metrics: ModelMetrics, task_type: str) -> float:
        """Calculate task-specific score for a model"""
        base_score = metrics.health_score
        if task_type == 'coding' and 'code' in metrics.name.lower():
            base_score += 0.2
        elif task_type == 'creative' and 'creative' in metrics.name.lower():
            base_score += 0.2
        elif task_type == 'analysis' and 'analysis' in metrics.name.lower():
            base_score += 0.2
        return min(1.0, base_score)

    def validate_all_models(self, model_list: List[str]=None) -> Dict[str, Any]:
        """Validate all models or a specific list"""
        if model_list is None:
            try:
                with open(self.config_path, 'r') as f:
                    data = json.load(f)
                    model_list = [result['name'] for result in data.get('results', [])]
            except:
                model_list = []
        results = {}
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            future_to_model = {executor.submit(self.validate_model, model): model for model in model_list}
            for future in concurrent.futures.as_completed(future_to_model):
                model = future_to_model[future]
                try:
                    (success, response_time, error_msg) = future.result()
                    results[model] = {'working': success, 'response_time': response_time, 'error': error_msg if not success else None, 'health_score': self.metrics.get(model, ModelMetrics(name=model, response_times=[], success_rate=0, error_count=0, total_requests=0, last_success=None, last_failure=None, quality_score=0, cost_per_request=0, capabilities=[], health_score=0)).health_score}
                except Exception as e:
                    results[model] = {'working': False, 'response_time': 0, 'error': str(e), 'health_score': 0.0}
        self._update_working_models_file(results)
        return results

    def _update_working_models_file(self, validation_results: Dict[str, Any]):
        """Update the working models file with latest validation results"""
        try:
            if self.config_path.exists():
                with open(self.config_path, 'r') as f:
                    data = json.load(f)
            else:
                data = {'working_models': [], 'results': []}
            existing_results = {r['name']: r for r in data['results']}
            for (model_name, result) in validation_results.items():
                existing_results[model_name] = {'name': model_name, 'working': result['working'], 'response_time': result['response_time'], 'quality': self.metrics.get(model_name, ModelMetrics(name=model_name, response_times=[], success_rate=0, error_count=0, total_requests=0, last_success=None, last_failure=None, quality_score=0.5, cost_per_request=0, capabilities=[], health_score=0)).quality_score, 'error': result.get('error'), 'health_score': result['health_score']}
            data['results'] = list(existing_results.values())
            data['working_models'] = [name for (name, result) in existing_results.items() if result['working'] and result['health_score'] > 0.5]
            data['validation_timestamp'] = time.time()
            data['total_working'] = len(data['working_models'])
            with open(self.config_path, 'w') as f:
                json.dump(data, f, indent=2, default=str)
        except Exception as e:
            logger.error(f'Failed to update working models file: {e}')

    def start_monitoring(self):
        """Start continuous model monitoring"""
        if self._monitoring:
            return
        self._monitoring = True
        self._monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self._monitor_thread.start()
        logger.info('Started model monitoring')

    def stop_monitoring(self):
        """Stop continuous monitoring"""
        self._monitoring = False
        if self._monitor_thread:
            self._monitor_thread.join(timeout=5)
        logger.info('Stopped model monitoring')

    def _monitor_loop(self):
        """Main monitoring loop"""
        while self._monitoring:
            try:
                unhealthy_models = [name for (name, metrics) in self.metrics.items() if metrics.health_score < 0.5]
                if unhealthy_models:
                    logger.info(f'Re-validating unhealthy models: {unhealthy_models}')
                    self.validate_all_models(unhealthy_models)
                time.sleep(self.health_check_interval)
            except Exception as e:
                logger.error(f'Error in monitoring loop: {e}')
                time.sleep(60)

    def get_model_status(self) -> Dict[str, Any]:
        """Get comprehensive status of all models"""
        with self._lock:
            status = {'total_models': len(self.metrics), 'healthy_models': len([m for m in self.metrics.values() if m.health_score > 0.7]), 'unhealthy_models': len([m for m in self.metrics.values() if m.health_score < 0.5]), 'monitoring_active': self._monitoring, 'auto_switch_enabled': self.auto_switch_enabled, 'models': {}}
            for (name, metrics) in self.metrics.items():
                status['models'][name] = {'health_score': metrics.health_score, 'success_rate': metrics.success_rate, 'avg_response_time': statistics.mean(metrics.response_times) if metrics.response_times else 0, 'quality_score': metrics.quality_score, 'total_requests': metrics.total_requests, 'last_success': metrics.last_success, 'capabilities': metrics.capabilities}
            return status

    def should_switch_model(self, current_model: str, performance_threshold: float=0.5) -> Optional[str]:
        """Determine if we should switch to a different model"""
        if not self.auto_switch_enabled:
            return None
        current_metrics = self.metrics.get(current_model)
        if not current_metrics or current_metrics.health_score < performance_threshold:
            return self.get_best_model(exclude_models=[current_model])
        return None
_validator_instance = None

def get_model_validator() -> EnhancedModelValidator:
    """Get the singleton model validator instance"""
    global _validator_instance
    if _validator_instance is None:
        _validator_instance = EnhancedModelValidator()
    return _validator_instance