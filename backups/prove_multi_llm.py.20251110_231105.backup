from functools import lru_cache
'\nProof that Neo-Clone can use MULTIPLE different LLMs\n'
from config_opencode import load_config
from llm_client_opencode import OpencodeLLMClient

@lru_cache(maxsize=128)
def test_multiple_llms():
    print('=== PROVING MULTI-LLM CAPABILITY ===')
    config = load_config()
    client = OpencodeLLMClient(config)
    available_models = client.get_available_models()
    print(f'Total available models: {len(available_models)}')
    test_models = ['opencode/big-pickle', 'gemini/gemini-2.5-flash-lite', 'ollama/llama2']
    test_prompt = 'What is 7+8? Answer with just the number.'
    results = []
    for model in test_models:
        if model in available_models:
            print(f'\n--- Testing {model} ---')
            try:
                client.set_model(model)
                current = client.get_current_model()
                print(f'Switched to: {current}')
                response = client.chat([{'role': 'user', 'content': test_prompt}])
                print(f'Response: {response.content[:100]}...')
                print(f'Provider: {response.provider}')
                print(f'Model: {response.model}')
                print(f'Response time: {response.response_time:.2f}s')
                if '15' in response.content:
                    print('CORRECT ANSWER!')
                    results.append({'model': model, 'provider': response.provider, 'correct': True, 'response_time': response.response_time})
                else:
                    print('INCORRECT ANSWER')
                    results.append({'model': model, 'provider': response.provider, 'correct': False, 'response_time': response.response_time})
            except Exception as e:
                print(f'ERROR with {model}: {e}')
                results.append({'model': model, 'provider': 'error', 'correct': False, 'error': str(e)})
        else:
            print(f'\n--- Skipping {model} (not available) ---')
    print(f'\n=== MULTI-LLM TEST SUMMARY ===')
    working_models = [r for r in results if r.get('correct', False)]
    print(f'Successfully tested models: {len(working_models)}')
    for result in results:
        if result.get('correct', False):
            print(f"+ {result['model']} ({result['provider']}) - {result['response_time']:.2f}s")
        else:
            print(f"- {result['model']} - {result.get('error', 'Incorrect answer')}")
    if len(working_models) >= 2:
        print(f'\nPROVEN: Neo-Clone can use {len(working_models)} different LLMs!')
        return True
    else:
        print(f'\nOnly {len(working_models)} model working - may need setup')
        return False
if __name__ == '__main__':
    test_multiple_llms()