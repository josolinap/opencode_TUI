from functools import lru_cache
'\nautonomous_intelligence.py - Neo-Clone Autonomous Intelligence System\n\nThis module implements self-learning and improvement capabilities for Neo-Clone,\nenabling the system to learn from interactions and continuously improve performance.\n'
import json
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import defaultdict, deque
import hashlib
from pathlib import Path
logger = logging.getLogger(__name__)

@dataclass
class LearningMetric:
    """Tracks learning metrics for continuous improvement"""
    timestamp: float
    task_type: str
    model_used: str
    success: bool
    response_time: float
    user_satisfaction: Optional[float] = None
    context_quality: Optional[float] = None
    improvement_suggestion: Optional[str] = None

@dataclass
class PerformancePattern:
    """Represents a learned performance pattern"""
    pattern_id: str
    task_pattern: str
    optimal_model: str
    confidence_score: float
    success_rate: float
    avg_response_time: float
    last_updated: float
    usage_count: int

@dataclass
class ContextMemory:
    """Tracks conversation context for better routing"""
    conversation_id: str
    messages: List[Dict[str, Any]]
    intent_history: List[str]
    skill_usage: Dict[str, int]
    performance_scores: List[float]
    context_quality_score: float

class AutonomousIntelligence:
    """Autonomous intelligence system for Neo-Clone"""

    def __init__(self, storage_path: str='autonomous_memory.json'):
        self.storage_path = Path(storage_path)
        self.learning_metrics: List[LearningMetric] = []
        self.performance_patterns: Dict[str, PerformancePattern] = {}
        self.context_memories: Dict[str, ContextMemory] = {}
        self.model_preferences: Dict[str, float] = {}
        self.skill_effectiveness: Dict[str, float] = {}
        self.learning_rate = 0.1
        self.pattern_confidence_threshold = 0.7
        self.max_memory_size = 1000
        self.context_window = 10
        self.session_start = time.time()
        self.total_interactions = 0
        self.successful_interactions = 0
        self.improvement_cycles = 0
        self._load_knowledge()

    def _load_knowledge(self):
        """Load learned knowledge from storage"""
        try:
            if self.storage_path.exists():
                with open(self.storage_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                self.learning_metrics = [LearningMetric(**m) for m in data.get('learning_metrics', [])]
                self.performance_patterns = {pid: PerformancePattern(**pp) for (pid, pp) in data.get('performance_patterns', {}).items()}
                self.context_memories = {cid: ContextMemory(**cm) for (cid, cm) in data.get('context_memories', {}).items()}
                self.model_preferences = data.get('model_preferences', {})
                self.skill_effectiveness = data.get('skill_effectiveness', {})
                logger.info(f'Loaded {len(self.learning_metrics)} learning metrics')
                logger.info(f'Loaded {len(self.performance_patterns)} performance patterns')
        except Exception as e:
            logger.warning(f'Failed to load autonomous knowledge: {e}')

    def _save_knowledge(self):
        """Save learned knowledge to storage"""
        try:
            data = {'learning_metrics': [asdict(m) for m in self.learning_metrics[-self.max_memory_size:]], 'performance_patterns': {pid: asdict(pp) for (pid, pp) in self.performance_patterns.items()}, 'context_memories': {cid: asdict(cm) for (cid, cm) in self.context_memories.items()}, 'model_preferences': self.model_preferences, 'skill_effectiveness': self.skill_effectiveness, 'last_updated': time.time(), 'improvement_cycles': self.improvement_cycles}
            with open(self.storage_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, default=str)
        except Exception as e:
            logger.error(f'Failed to save autonomous knowledge: {e}')

    @lru_cache(maxsize=128)
    def record_interaction(self, task_type: str, model_used: str, success: bool, response_time: float, context: Dict[str, Any]=None) -> str:
        """Record an interaction for learning"""
        metric = LearningMetric(timestamp=time.time(), task_type=task_type, model_used=model_used, success=success, response_time=response_time, user_satisfaction=context.get('user_satisfaction') if context else None, context_quality=context.get('context_quality') if context else None)
        self.learning_metrics.append(metric)
        self.total_interactions += 1
        if success:
            self.successful_interactions += 1
        self._update_model_preferences(model_used, success, response_time)
        if context and 'skill_used' in context:
            self._update_skill_effectiveness(context['skill_used'], success, response_time)
        if context and 'conversation_id' in context:
            self._update_context_memory(context['conversation_id'], task_type, success, context)
        if self.total_interactions % 10 == 0:
            self._learning_cycle()
        return self._generate_interaction_id(metric)

    def _update_model_preferences(self, model: str, success: bool, response_time: float):
        """Update model preference scores"""
        if model not in self.model_preferences:
            self.model_preferences[model] = 0.5
        success_bonus = 0.1 if success else -0.2
        speed_bonus = max(0, 1.0 - response_time / 5.0) * 0.1
        self.model_preferences[model] += self.learning_rate * (success_bonus + speed_bonus)
        self.model_preferences[model] = max(0.1, min(1.0, self.model_preferences[model]))

    def _update_skill_effectiveness(self, skill: str, success: bool, response_time: float):
        """Update skill effectiveness scores"""
        if skill not in self.skill_effectiveness:
            self.skill_effectiveness[skill] = 0.5
        success_bonus = 0.1 if success else -0.15
        speed_bonus = max(0, 1.0 - response_time / 3.0) * 0.1
        self.skill_effectiveness[skill] += self.learning_rate * (success_bonus + speed_bonus)
        self.skill_effectiveness[skill] = max(0.1, min(1.0, self.skill_effectiveness[skill]))

    def _update_context_memory(self, conversation_id: str, task_type: str, success: bool, context: Dict[str, Any]):
        """Update conversation context memory"""
        if conversation_id not in self.context_memories:
            self.context_memories[conversation_id] = ContextMemory(conversation_id=conversation_id, messages=[], intent_history=[], skill_usage=defaultdict(int), performance_scores=[], context_quality_score=0.0)
        memory = self.context_memories[conversation_id]
        memory.intent_history.append(task_type)
        if success:
            memory.performance_scores.append(1.0)
        else:
            memory.performance_scores.append(0.0)
        memory.performance_scores = memory.performance_scores[-self.context_window:]
        if memory.performance_scores:
            memory.context_quality_score = sum(memory.performance_scores) / len(memory.performance_scores)

    def _learning_cycle(self):
        """Perform a learning cycle to improve patterns"""
        logger.info('Starting autonomous learning cycle...')
        self._analyze_performance_patterns()
        self._optimize_model_selection()
        self._improve_routing_logic()
        self._cleanup_old_data()
        self.improvement_cycles += 1
        self._save_knowledge()
        logger.info(f'Learning cycle {self.improvement_cycles} completed')

    def _analyze_performance_patterns(self):
        """Analyze performance to identify patterns"""
        recent_metrics = self.learning_metrics[-50:]
        task_patterns = defaultdict(list)
        for metric in recent_metrics:
            pattern = self._extract_task_pattern(metric.task_type)
            task_patterns[pattern].append(metric)
        for (pattern, metrics) in task_patterns.items():
            if len(metrics) < 3:
                continue
            pattern_id = hashlib.md5(pattern.encode()).hexdigest()[:8]
            model_performance = defaultdict(list)
            for metric in metrics:
                model_performance[metric.model_used].append(metric)
            best_model = None
            best_score = -1
            for (model, model_metrics) in model_performance.items():
                success_rate = sum((1 for m in model_metrics if m.success)) / len(model_metrics)
                avg_response_time = sum((m.response_time for m in model_metrics)) / len(model_metrics)
                score = success_rate * 0.7 + max(0, 1 - avg_response_time / 3.0) * 0.3
                if score > best_score:
                    best_score = score
                    best_model = model
            if pattern_id in self.performance_patterns:
                existing = self.performance_patterns[pattern_id]
                existing.confidence_score = min(1.0, existing.confidence_score + 0.1)
                existing.last_updated = time.time()
                existing.usage_count += len(metrics)
            else:
                self.performance_patterns[pattern_id] = PerformancePattern(pattern_id=pattern_id, task_pattern=pattern, optimal_model=best_model, confidence_score=0.5, success_rate=best_score, avg_response_time=sum((m.response_time for m in metrics)) / len(metrics), last_updated=time.time(), usage_count=len(metrics))

    def _extract_task_pattern(self, task_type: str) -> str:
        """Extract generalized pattern from task type"""
        task_lower = task_type.lower()
        if any((word in task_lower for word in ['code', 'python', 'javascript', 'function'])):
            return 'code_generation'
        elif any((word in task_lower for word in ['analyze', 'explain', 'understand'])):
            return 'analysis'
        elif any((word in task_lower for word in ['write', 'create', 'generate'])):
            return 'content_creation'
        elif any((word in task_lower for word in ['debug', 'fix', 'error'])):
            return 'troubleshooting'
        elif any((word in task_lower for word in ['data', 'csv', 'json'])):
            return 'data_processing'
        else:
            return 'general'

    def _optimize_model_selection(self):
        """Optimize model selection based on learned preferences"""
        sorted_models = sorted(self.model_preferences.items(), key=lambda x: x[1], reverse=True)
        logger.info(f'Updated model preferences: {sorted_models[:5]}')

    def _improve_routing_logic(self):
        """Improve routing decisions based on context"""
        successful_contexts = [cm for cm in self.context_memories.values() if cm.context_quality_score > 0.8]
        if successful_contexts:
            common_intents = defaultdict(int)
            for context in successful_contexts:
                for intent in context.intent_history[-5:]:
                    common_intents[intent] += 1
            logger.info(f'Identified successful routing patterns: {dict(common_intents)}')

    def _cleanup_old_data(self):
        """Clean up old learning data"""
        cutoff_time = time.time() - 30 * 24 * 3600
        self.learning_metrics = [m for m in self.learning_metrics if m.timestamp > cutoff_time]
        old_contexts = [cid for (cid, cm) in self.context_memories.items() if cm.messages and cm.messages[0].get('timestamp', 0) < cutoff_time]
        for cid in old_contexts:
            del self.context_memories[cid]
        logger.info(f'Cleaned up {len(old_contexts)} old context memories')

    def _generate_interaction_id(self, metric: LearningMetric) -> str:
        """Generate unique interaction ID"""
        content = f'{metric.timestamp}_{metric.task_type}_{metric.model_used}'
        return hashlib.md5(content.encode()).hexdigest()[:12]

    def get_optimal_model(self, task_type: str, context: Dict[str, Any]=None) -> Tuple[str, float]:
        """Get optimal model for a given task based on learning"""
        pattern = self._extract_task_pattern(task_type)
        pattern_id = hashlib.md5(pattern.encode()).hexdigest()[:8]
        if pattern_id in self.performance_patterns:
            learned_pattern = self.performance_patterns[pattern_id]
            if learned_pattern.confidence_score >= self.pattern_confidence_threshold:
                logger.info(f'Using learned pattern: {pattern} -> {learned_pattern.optimal_model}')
                return (learned_pattern.optimal_model, learned_pattern.confidence_score)
        if self.model_preferences:
            best_model = max(self.model_preferences.items(), key=lambda x: x[1])
            return (best_model[0], best_model[1])
        return ('opencode/big-pickle', 0.5)

    def get_context_aware_routing(self, conversation_id: str, current_task: str) -> Dict[str, Any]:
        """Get context-aware routing recommendations"""
        if conversation_id not in self.context_memories:
            return {'recommendation': 'default_routing', 'confidence': 0.5, 'reason': 'No context history available'}
        memory = self.context_memories[conversation_id]
        if memory.performance_scores:
            recent_performance = sum(memory.performance_scores[-3:]) / min(3, len(memory.performance_scores))
        else:
            recent_performance = 0.5
        pattern = self._extract_task_pattern(current_task)
        pattern_success = memory.intent_history.count(pattern) > 0
        recent_pattern_success = any((intent == pattern for intent in memory.intent_history[-5:]))
        recommendations = []
        if recent_performance > 0.8 and recent_pattern_success:
            recommendations.append({'action': 'continue_current_strategy', 'confidence': recent_performance, 'reason': 'Recent performance is strong'})
        else:
            recommendations.append({'action': 'try_alternative_model', 'confidence': 1.0 - recent_performance, 'reason': 'Performance could be improved'})
        return {'current_performance': recent_performance, 'pattern_success': pattern_success, 'recommendations': recommendations, 'context_quality': memory.context_quality_score, 'conversation_length': len(memory.intent_history)}

    def get_learning_insights(self) -> Dict[str, Any]:
        """Get insights about what the system has learned"""
        if not self.learning_metrics:
            return {'status': 'no_data', 'message': 'No learning data available'}
        recent_metrics = self.learning_metrics[-100:]
        success_rate = sum((1 for m in recent_metrics if m.success)) / len(recent_metrics)
        avg_response_time = sum((m.response_time for m in recent_metrics)) / len(recent_metrics)
        model_success = defaultdict(lambda : [0, 0])
        for metric in recent_metrics:
            model_success[metric.model_used][1] += 1
            if metric.success:
                model_success[metric.model_used][0] += 1
        model_performance = {}
        for (model, (successes, total)) in model_success.items():
            if total > 0:
                model_performance[model] = successes / total
        total_patterns = len(self.performance_patterns)
        confident_patterns = sum((1 for p in self.performance_patterns.values() if p.confidence_score >= self.pattern_confidence_threshold))
        return {'learning_statistics': {'total_interactions': self.total_interactions, 'success_rate': success_rate, 'avg_response_time': avg_response_time, 'improvement_cycles': self.improvement_cycles}, 'model_performance': model_performance, 'learned_patterns': {'total_patterns': total_patterns, 'confident_patterns': confident_patterns, 'patterns': list(self.performance_patterns.values())[:5]}, 'context_awareness': {'active_conversations': len(self.context_memories), 'avg_context_quality': sum((cm.context_quality_score for cm in self.context_memories.values())) / len(self.context_memories) if self.context_memories else 0}, 'skill_effectiveness': self.skill_effectiveness}

    def force_learning_cycle(self):
        """Force an immediate learning cycle"""
        logger.info('Forcing immediate learning cycle...')
        self._learning_cycle()

    def reset_learning(self):
        """Reset all learning data"""
        logger.warning('Resetting all autonomous learning data...')
        self.learning_metrics.clear()
        self.performance_patterns.clear()
        self.context_memories.clear()
        self.model_preferences.clear()
        self.skill_effectiveness.clear()
        self.total_interactions = 0
        self.successful_interactions = 0
        self.improvement_cycles = 0
        self._save_knowledge()
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    ai = AutonomousIntelligence()
    interactions = [('code_generation', 'opencode/grok-code', True, 2.1), ('analysis', 'opencode/big-pickle', True, 1.5), ('code_generation', 'opencode/grok-code', True, 1.8), ('content_creation', 'opencode/big-pickle', True, 2.3), ('analysis', 'opencode/big-pickle', False, 5.2)]
    for (task_type, model, success, response_time) in interactions:
        interaction_id = ai.record_interaction(task_type=task_type, model_used=model, success=success, response_time=response_time, context={'conversation_id': 'test_conv', 'skill_used': task_type})
        print(f'Recorded interaction {interaction_id}: {task_type} with {model}')
    (optimal_model, confidence) = ai.get_optimal_model('code_generation')
    print(f'Optimal model for code_generation: {optimal_model} (confidence: {confidence:.2f})')
    insights = ai.get_learning_insights()
    print('Learning Insights:', json.dumps(insights, indent=2))