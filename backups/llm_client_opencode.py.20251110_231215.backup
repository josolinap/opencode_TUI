from functools import lru_cache
"\nllm_client_opencode.py - Opencode-compatible LLM client for Neo-Clone\n\nThis module provides an enhanced LLM client that works seamlessly with Opencode's\nmodel selection system while maintaining backward compatibility with Neo-Clone's\noriginal LLM client pattern.\n\nFeatures:\n- Integrates with Opencode's model selection\n- Supports provider/model format translation\n- Maintains single-instance client pattern\n- Graceful fallback when Opencode is unavailable\n- Supports multiple LLM providers (Ollama, OpenAI, Anthropic, etc.)\n"
import logging
import requests
import json
import subprocess
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from config_opencode import Config, get_current_opencode_model, is_opencode_available
import time
import os
import sys
try:
    from skills.free_model_scanner import FreeModelScanner, FreeModel
except ImportError:
    FreeModelScanner = None
    FreeModel = None
logger = logging.getLogger(__name__)

@dataclass
class LLMResponse:
    """Standardized LLM response format"""
    content: str
    model: str
    provider: str
    usage: Optional[Dict[str, int]] = None
    finish_reason: Optional[str] = None
    response_time: float = 0.0

class OpencodeLLMClient:
    """Enhanced LLM client with Opencode integration"""

    def __init__(self, cfg: Config):
        self.cfg = cfg
        self.session = requests.Session()
        self._model_cache = {}
        self._cache_timestamp = 0
        self.available_models = self._discover_available_models()
        self.current_model = self._initialize_model()

    @lru_cache(maxsize=128)
    def _discover_available_models(self) -> List[str]:
        """Use the same free models that Opencode provides in its /models"""
        models = []
        opencode_free_models = ['opencode/big-pickle', 'gemini/gemini-2.5-flash-lite', 'gemini/gemini-2.5-flash', 'ollama/llama2', 'ollama/codellama']
        models.extend(opencode_free_models)
        logger.info(f'Using {len(opencode_free_models)} verified free models from Opencode ecosystem')
        working_models = self._test_working_models(models)
        additional_free_models = ['gemini/gemini-2.5-flash-lite', 'gemini/gemini-2.5-flash', 'gemini/gemini-1.5-flash', 'gemini/gemini-1.5-pro']
        for model in additional_free_models:
            if model not in working_models:
                working_models.append(model)
        if FreeModelScanner:
            try:
                scanner = FreeModelScanner()
                discovered_models = scanner.discover_all_free_models()
                internet_models = []
                for model in discovered_models:
                    if not model.requires_api_key or 'gemini' in model.name.lower() or 'ollama' in model.name.lower() or ('opencode' in model.name.lower()):
                        internet_models.append(model.name)
                for model_name in internet_models:
                    if model_name not in working_models:
                        working_models.append(model_name)
                logger.info(f'Added {len(internet_models)} free models from internet scanning')
            except Exception as e:
                logger.warning(f'Failed to scan internet for free models: {e}')
        if working_models:
            working_models = self._prioritize_working_models(working_models)
        if not working_models:
            working_models = ['opencode/big-pickle', 'opencode/grok-code', 'gemini/gemini-2.5-flash-lite', 'gemini/gemini-2.5-flash', 'ollama/llama2', 'ollama/codellama']
            logger.info('Using fallback working model list')
        return working_models

    def _get_cached_model_info(self, model: str) -> Optional[Dict[str, Any]]:
        """Get cached model information to avoid repeated API calls"""
        current_time = time.time()
        if current_time - self._cache_timestamp > 300:
            self._model_cache = {}
            self._cache_timestamp = current_time
        return self._model_cache.get(model)

    def _cache_model_info(self, model: str, info: Dict[str, Any]):
        """Cache model information"""
        self._model_cache[model] = info
        self._cache_timestamp = time.time()

    def _parse_models_dev_response(self, data: Dict[str, Any]) -> List[str]:
        """Parse the models.dev API response (same format as Opencode uses)"""
        models = []
        try:
            for (provider_id, provider_data) in data.items():
                if isinstance(provider_data, dict) and 'models' in provider_data:
                    provider_models = provider_data['models']
                    if isinstance(provider_models, dict):
                        for (model_id, model_info) in provider_models.items():
                            if isinstance(model_info, dict):
                                model_name = f'{provider_id}/{model_id}'
                                if 'cost' in model_info:
                                    cost = model_info['cost']
                                    if isinstance(cost, dict) and 'input' in cost:
                                        input_cost = cost['input']
                                        if input_cost == 0 or input_cost < 0.0001:
                                            models.append(model_name)
                                elif model_info.get('experimental') == False or 'free' in model_id.lower():
                                    models.append(model_name)
            additional_free = ['opencode/big-pickle', 'opencode/grok-code', 'gemini/gemini-2.5-flash-lite', 'gemini/gemini-2.5-flash', 'ollama/llama2', 'ollama/codellama']
            for model in additional_free:
                if model not in models:
                    models.append(model)
            logger.info(f'Parsed {len(models)} free models from models.dev API')
            return models
        except Exception as e:
            logger.warning(f'Failed to parse models.dev response: {e}')
            return []

    def _parse_opencode_models_response(self, response_text: str) -> List[str]:
        """Parse the models response from Opencode's running instance"""
        models = []
        try:
            import json
            data = json.loads(response_text)
            if isinstance(data, list):
                models = [str(model) for model in data if model]
            elif isinstance(data, dict):
                if 'models' in data:
                    models_data = data['models']
                    if isinstance(models_data, list):
                        models = [str(model) for model in models_data if model]
                    elif isinstance(models_data, dict):
                        models = list(models_data.keys())
                else:
                    for key in ['available_models', 'data', 'result']:
                        if key in data and isinstance(data[key], list):
                            models = [str(model) for model in data[key] if model]
                            break
            free_models = []
            for model in models:
                if not self._is_likely_paid_model(model):
                    free_models.append(model)
            logger.info(f'Parsed {len(free_models)} free models from Opencode response')
            return free_models
        except json.JSONDecodeError:
            lines = response_text.split('\n')
            for line in lines:
                line = line.strip()
                if line and '/' in line and (len(line) > 3):
                    if not self._is_likely_paid_model(line):
                        models.append(line)
            logger.info(f'Parsed {len(models)} models from text response')
            return models
        except Exception as e:
            logger.warning(f'Failed to parse Opencode models response: {e}')
            return []

    def _prioritize_free_models(self, models: List[str]) -> List[str]:
        """Prioritize free models based on accessibility and quality"""
        free_model_priorities = {'opencode/big-pickle': 1, 'opencode/grok-code': 2, 'gemini/gemini-2.5-flash-lite': 3, 'gemini/gemini-2.5-flash': 4, 'gemini/gemini-1.5-flash': 5, 'gemini/gemini-1.5-pro': 6, 'openai/gpt-3.5-turbo': 7, 'openai/gpt-4o-mini': 8, 'openai/text-embedding-3-small': 9, 'openai/text-embedding-ada-002': 10, 'ollama/llama2': 11, 'ollama/codellama': 12, 'ollama/mistral': 13}

        def get_priority(model: str) -> int:
            return free_model_priorities.get(model, 999)
        sorted_models = sorted(models, key=get_priority)
        prioritized_free = [m for m in sorted_models if get_priority(m) < 999]
        logger.info(f'Prioritized {len(prioritized_free)} free models out of {len(models)} total')
        return sorted_models

    def _test_working_models(self, models: List[str]) -> List[str]:
        """Test which models are actually working by making a minimal request"""
        working_models = []
        definitely_free_models = []
        for model in models:
            if model.startswith('opencode/') or model.startswith('gemini/') or model.startswith('ollama/') or model.startswith('huggingface/') or ('free' in model.lower()) or ('open' in model.lower()):
                definitely_free_models.append(model)
            else:
                logger.info(f'âœ— {model} - Filtered out (likely paid/API-key required)')
        opencode_models = [m for m in definitely_free_models if m.startswith('opencode/')]
        gemini_models = [m for m in definitely_free_models if m.startswith('gemini/')]
        ollama_models = [m for m in definitely_free_models if m.startswith('ollama/')]
        huggingface_models = [m for m in definitely_free_models if m.startswith('huggingface/')]
        for model in opencode_models:
            if model == 'opencode/big-pickle':
                working_models.append(model)
                logger.info(f'+ {model} - Verified working')
            elif self._quick_test_model(model):
                working_models.append(model)
                logger.info(f'+ {model} - Working')
            else:
                logger.warning(f'- {model} - Not working')
        for model in gemini_models:
            if self._quick_test_model(model):
                working_models.append(model)
                logger.info(f'+ {model} - Working')
            else:
                logger.info(f'- {model} - CLI not available')
        for model in ollama_models:
            if self._quick_test_model(model):
                working_models.append(model)
                logger.info(f'+ {model} - Working')
            else:
                logger.info(f'- {model} - Ollama not running')
        working_models.extend(huggingface_models[:10])
        logger.info(f'+ Added {min(10, len(huggingface_models))} HuggingFace models')
        return working_models

    def _quick_test_model(self, model: str) -> bool:
        """Quick test if a model works with a minimal request"""
        try:
            if any((paid_provider in model for paid_provider in ['openai/gpt-4', 'openai/gpt-5', 'openai/o1', 'openai/o3', 'openai/o4', 'anthropic/claude-3', 'google/gemini-pro'])):
                return False
            cmd = ['opencode', 'run', '--model', model, '--format', 'json', 'hi']
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=3)
            if result.returncode == 0:
                if '"type":"text"' in result.stdout or 'text' in result.stdout:
                    return True
            elif result.returncode != 0:
                stderr = result.stderr.lower()
                if any((error in stderr for error in ['api key', 'invalid_api_key', '401', 'unauthorized'])):
                    return False
            return False
        except subprocess.TimeoutExpired:
            logger.debug(f'Model {model} timed out (might be working but slow)')
            return False
        except Exception as e:
            logger.debug(f'Error testing model {model}: {e}')
            return False

    def _prioritize_working_models(self, models: List[str]) -> List[str]:
        """Prioritize working models based on verified accessibility and quality"""
        working_model_priorities = {'opencode/big-pickle': 1, 'opencode/grok-code': 2, 'gemini/gemini-2.5-flash-lite': 3, 'gemini/gemini-2.5-flash': 4, 'gemini/gemini-1.5-flash': 5, 'gemini/gemini-1.5-pro': 6, 'ollama/llama2': 7, 'ollama/codellama': 8, 'ollama/mistral': 9, 'ollama/phi': 10, 'groq/llama3-8b-8192': 11, 'together.ai/meta-llama/Llama-2-7b-chat-hf': 12}

        def get_priority(model: str) -> int:
            return working_model_priorities.get(model, 999)
        sorted_models = sorted(models, key=get_priority)
        prioritized_working = [m for m in sorted_models if get_priority(m) < 999]
        logger.info(f'Prioritized {len(prioritized_working)} verified working models out of {len(models)} total')
        return sorted_models

    def _is_likely_paid_model(self, model: str) -> bool:
        """Check if a model is likely paid or requires API keys"""
        model_lower = model.lower()
        paid_patterns = ['gpt-4', 'gpt-5', 'o1', 'o3', 'o4', 'claude-3', 'gemini-pro', 'text-embedding', 'dall-e', 'midjourney', 'stable-diffusion']
        for pattern in paid_patterns:
            if pattern in model_lower:
                return True
        free_exceptions = ['opencode/big-pickle', 'opencode/grok-code', 'openai/gpt-3.5-turbo', 'openai/gpt-4o-mini', 'openai/text-embedding-3-small', 'openai/text-embedding-ada-002']
        if model in free_exceptions:
            return False
        if model.startswith('opencode/'):
            return False
        return False

    def _initialize_model(self) -> str:
        """Initialize the current model from config or Opencode"""
        opencode_model = get_current_opencode_model()
        if opencode_model:
            logger.info(f'Using Opencode model: {opencode_model}')
            return opencode_model
        if self.cfg.opencode_model:
            logger.info(f'Using configured Opencode model: {self.cfg.opencode_model}')
            return self.cfg.opencode_model
        return f'{self.cfg.provider}/{self.cfg.model_name}'

    def _get_api_client(self, provider: str) -> 'APIAdapter':
        """Get appropriate API adapter for the provider"""
        adapters = {'ollama': OllamaAdapter(self.cfg, self.session), 'openai': OpencodeAdapter(self.cfg, self.session), 'anthropic': OpencodeAdapter(self.cfg, self.session), 'opencode': OpencodeAdapter(self.cfg, self.session), 'gemini': GeminiAdapter(self.cfg, self.session), 'google': GeminiAdapter(self.cfg, self.session), 'api': OpencodeAdapter(self.cfg, self.session)}
        return adapters.get(provider, OpencodeAdapter(self.cfg, self.session))

    def chat(self, messages: List[Dict[str, str]], timeout: int=30) -> LLMResponse:
        """Send chat request with timing and enhanced error handling"""
        start_time = time.time()
        try:
            if not messages or not isinstance(messages, list):
                raise ValueError('Messages must be a non-empty list')
            if not all((isinstance(msg, dict) and 'role' in msg and ('content' in msg) for msg in messages)):
                raise ValueError("Each message must be a dict with 'role' and 'content' keys")
            if '/' in self.current_model:
                (provider, model) = self.current_model.split('/', 1)
            else:
                (provider, model) = (self.cfg.provider, self.current_model)
            adapter = self._get_api_client(provider)
            response = adapter.chat(messages, self.current_model, timeout)
            response.response_time = time.time() - start_time
            logger.info(f'LLM request completed: {provider}/{model} ({response.response_time:.2f}s)')
            return response
        except ValueError as e:
            response_time = time.time() - start_time
            error_msg = f'[Neo Error] Invalid input: {str(e)}'
            logger.error(f'Input validation failed: {e}')
            return LLMResponse(content=error_msg, model=self.current_model, provider='error', response_time=response_time)
        except ConnectionError as e:
            response_time = time.time() - start_time
            error_msg = f'[Neo Error] Connection failed: {str(e)}'
            logger.error(f'Connection error: {e}')
            return LLMResponse(content=error_msg, model=self.current_model, provider='error', response_time=response_time)
        except TimeoutError as e:
            response_time = time.time() - start_time
            error_msg = f'[Neo Error] Request timed out after {timeout}s'
            logger.error(f'Timeout error: {e}')
            return LLMResponse(content=error_msg, model=self.current_model, provider='error', response_time=response_time)
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = f'[Neo Error] LLM request failed: {str(e)}'
            logger.error(f'LLM request failed: {e}')
            return LLMResponse(content=error_msg, model=self.current_model, provider='error', response_time=response_time)

    def set_model(self, model: str):
        """Set the current model (compatible with Opencode selection)"""
        if model in self.available_models:
            self.current_model = model
            logger.info(f'Model set to: {model}')
        else:
            logger.warning(f'Model {model} not in available models list')
            self.current_model = model

    def get_current_model(self) -> str:
        """Get the currently selected model"""
        return self.current_model

    def get_available_models(self) -> List[str]:
        """Get list of available models"""
        return self.available_models.copy()

    def refresh_models(self):
        """Refresh the list of available models"""
        self.available_models = self._discover_available_models()

class APIAdapter:
    """Base class for LLM API adapters"""

    def __init__(self, cfg: Config, session: requests.Session=None):
        self.cfg = cfg
        self.session = session or requests.Session()

    def chat(self, messages: List[Dict[str, str]], model: str, timeout: int) -> LLMResponse:
        raise NotImplementedError

class OllamaAdapter(APIAdapter):
    """Adapter for Ollama API"""

    def chat(self, messages: List[Dict[str, str]], model: str, timeout: int) -> LLMResponse:
        url = self.cfg.api_endpoint.rstrip('/') + '/api/chat'
        payload = {'model': model, 'messages': messages, 'max_tokens': self.cfg.max_tokens, 'temperature': self.cfg.temperature}
        resp = self.session.post(url, json=payload, timeout=timeout)
        resp.raise_for_status()
        try:
            data = resp.json()
        except ValueError as e:
            raw_response = resp.text
            import re
            cleaned_response = re.sub('[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]', '', raw_response)
            raise ValueError(f'Invalid JSON response from server: {e}\nRaw response: {cleaned_response[:500]}...')
        return LLMResponse(content=data.get('message', {}).get('content', 'No response.'), model=model, provider='ollama')

class OpenAIAdapter(APIAdapter):
    """Adapter for OpenAI API"""

    def chat(self, messages: List[Dict[str, str]], model: str, timeout: int) -> LLMResponse:
        if not self.cfg.api_key:
            raise ValueError('OpenAI API key required')
        url = 'https://api.openai.com/v1/chat/completions'
        headers = {'Authorization': f'Bearer {self.cfg.api_key}', 'Content-Type': 'application/json'}
        payload = {'model': model, 'messages': messages, 'max_tokens': self.cfg.max_tokens, 'temperature': self.cfg.temperature}
        resp = self.session.post(url, headers=headers, json=payload, timeout=timeout)
        resp.raise_for_status()
        try:
            data = resp.json()
        except ValueError as e:
            raw_response = resp.text
            import re
            cleaned_response = re.sub('[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]', '', raw_response)
            raise ValueError(f'Invalid JSON response from OpenAI: {e}\nRaw response: {cleaned_response[:500]}...')
        return LLMResponse(content=data['choices'][0]['message']['content'], model=model, provider='openai', usage=data.get('usage'), finish_reason=data['choices'][0].get('finish_reason'))

class AnthropicAdapter(APIAdapter):
    """Adapter for Anthropic API"""

    def chat(self, messages: List[Dict[str, str]], model: str, timeout: int) -> LLMResponse:
        if not self.cfg.api_key:
            raise ValueError('Anthropic API key required')
        url = 'https://api.anthropic.com/v1/messages'
        headers = {'x-api-key': self.cfg.api_key, 'Content-Type': 'application/json', 'anthropic-version': '2023-06-01'}
        system_msg = None
        anthropic_messages = []
        for msg in messages:
            if msg['role'] == 'system':
                system_msg = msg['content']
            else:
                anthropic_messages.append({'role': msg['role'], 'content': msg['content']})
        payload = {'model': model, 'max_tokens': self.cfg.max_tokens, 'messages': anthropic_messages}
        if system_msg:
            payload['system'] = system_msg
        resp = self.session.post(url, headers=headers, json=payload, timeout=timeout)
        resp.raise_for_status()
        try:
            data = resp.json()
        except ValueError as e:
            raw_response = resp.text
            import re
            cleaned_response = re.sub('[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]', '', raw_response)
            raise ValueError(f'Invalid JSON response from Anthropic: {e}\nRaw response: {cleaned_response[:500]}...')
        return LLMResponse(content=data['content'][0]['text'], model=model, provider='anthropic', usage=data.get('usage'), finish_reason=data.get('stop_reason'))

class OpencodeAdapter(APIAdapter):
    """Adapter for OpenCode CLI tool"""

    def chat(self, messages: List[Dict[str, str]], model: str, timeout: int) -> LLMResponse:
        """Use OpenCode CLI for completion"""
        try:
            prompt = ''
            for msg in reversed(messages):
                if msg['role'] == 'user':
                    prompt = msg['content']
                    break
            if not prompt:
                return LLMResponse(content='No user message found to process', model=model, provider='opencode')
            cmd = ['opencode', 'run', '--model', model, '--format', 'json']
            result = subprocess.run(cmd, input=prompt, capture_output=True, text=True, timeout=timeout)
            if result.returncode == 0:
                content = self._parse_opencode_json_output(result.stdout)
                if not content:
                    content = result.stdout.strip() if result.stdout.strip() else 'OpenCode returned empty response'
                return LLMResponse(content=content, model=model, provider='opencode')
            else:
                error_msg = result.stderr.strip() if result.stderr else f'OpenCode CLI failed with code {result.returncode}'
                return LLMResponse(content=f'[OpenCode Error] {error_msg}', model=model, provider='opencode')
        except subprocess.TimeoutExpired:
            return LLMResponse(content='[OpenCode Error] Request timed out', model=model, provider='opencode')
        except Exception as e:
            return LLMResponse(content=f'[OpenCode Error] {str(e)}', model=model, provider='opencode')

    def _parse_opencode_json_output(self, output: str) -> str:
        """Parse OpenCode JSON output to extract the actual response content"""
        try:
            import json
            lines = output.strip().split('\n')
            for line in lines:
                if line.strip():
                    try:
                        data = json.loads(line)
                        if data.get('type') == 'text' and 'part' in data:
                            text = data['part'].get('text', '')
                            if text.strip():
                                return text.strip()
                        elif 'content' in data:
                            return data['content']
                        elif 'message' in data and 'content' in data['message']:
                            return data['message']['content']
                        elif 'text' in data:
                            return data['text']
                    except json.JSONDecodeError:
                        continue
            return output
        except Exception:
            return output

class GeminiAdapter(APIAdapter):
    """Adapter for Gemini CLI tool"""

    def chat(self, messages: List[Dict[str, str]], model: str, timeout: int) -> LLMResponse:
        """Use Gemini CLI for completion"""
        try:
            prompt = ''
            for msg in reversed(messages):
                if msg['role'] == 'user':
                    prompt = msg['content']
                    break
            if not prompt:
                return LLMResponse(content='No user message found to process', model=model, provider='gemini')
            model_name = model.split('/')[-1] if '/' in model else model
            gemini_cmd = 'C:\\Users\\JO\\AppData\\Roaming\\npm\\gemini.cmd'
            cmd = [gemini_cmd, '--prompt', prompt]
            if model_name and model_name != 'gemini':
                cmd.extend(['--model', model_name])
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
            if result.returncode == 0:
                content = self._parse_gemini_json_output(result.stdout)
                if not content:
                    content = result.stdout.strip() if result.stdout.strip() else 'Gemini returned empty response'
                return LLMResponse(content=content, model=model, provider='gemini')
            else:
                error_msg = result.stderr.strip() if result.stderr else f'Gemini CLI failed with code {result.returncode}'
                return LLMResponse(content=f'[Gemini Error] {error_msg}', model=model, provider='gemini')
        except subprocess.TimeoutExpired:
            return LLMResponse(content='[Gemini Error] Request timed out', model=model, provider='gemini')
        except Exception as e:
            return LLMResponse(content=f'[Gemini Error] {str(e)}', model=model, provider='gemini')

    def _parse_gemini_json_output(self, output: str) -> str:
        """Parse Gemini CLI JSON output to extract the response"""
        try:
            import json
            data = json.loads(output.strip())
            if 'response' in data:
                return data['response']
            elif 'content' in data:
                return data['content']
            elif 'text' in data:
                return data['text']
            return str(data)
        except json.JSONDecodeError:
            return output.strip()
        except Exception:
            return output.strip()

class GenericAPIAdapter(APIAdapter):
    """Generic adapter for other API providers"""

    def chat(self, messages: List[Dict[str, str]], model: str, timeout: int) -> LLMResponse:
        if any((provider in model.lower() for provider in ['openai', 'anthropic', 'opencode'])):
            opencode_adapter = OpencodeAdapter(self.cfg, self.session)
            return opencode_adapter.chat(messages, model, timeout)
        raise ValueError(f'Generic API adapter not configured for provider: {self.cfg.provider}')

class LLMClient(OpencodeLLMClient):
    """Backward compatible LLM client"""
    pass