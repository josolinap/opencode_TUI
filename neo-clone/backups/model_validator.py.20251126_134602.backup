from functools import lru_cache
'\nModel Validation System for NEO-CLONE\nTests AI models and ensures they work properly with the brain system\n'
import json
import time
import requests
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import logging
logger = logging.getLogger(__name__)

@dataclass
class ValidationResult:
    """Result of model validation"""
    model_id: str
    is_valid: bool
    response_time: float
    capabilities_tested: List[str]
    capabilities_passed: List[str]
    error_message: str = ''
    score: float = 0.0

class ModelValidator:
    """Advanced model validation system"""

    def __init__(self):
        self.test_prompts = {'reasoning': 'Explain why the sky is blue in 2-3 sentences.', 'tool_calling': 'What is the current date? Please use a tool to check.', 'code_generation': 'Write a Python function to calculate fibonacci numbers.', 'math': 'What is 15 * 27 + 42?', 'creativity': 'Write a short poem about artificial intelligence.', 'analysis': 'Analyze the pros and cons of remote work.'}

    def validate_model_comprehensive(self, model_info, max_tests: int=3) -> ValidationResult:
        """Comprehensive validation of a model's capabilities"""
        start_time = time.time()
        capabilities_tested = []
        capabilities_passed = []
        (is_connected, connect_time, connect_error) = self.test_basic_connectivity(model_info)
        if not is_connected:
            return ValidationResult(model_id=f'{model_info.provider}/{model_info.model_name}', is_valid=False, response_time=time.time() - start_time, capabilities_tested=[], capabilities_passed=[], error_message=f'Connection failed: {connect_error}')
        for (capability, prompt) in list(self.test_prompts.items())[:max_tests]:
            capabilities_tested.append(capability)
            try:
                (is_capable, response) = self.test_capability(model_info, capability, prompt)
                if is_capable:
                    capabilities_passed.append(capability)
            except Exception as e:
                logger.warning(f'Capability test failed for {capability}: {e}')
        capability_score = len(capabilities_passed) / len(capabilities_tested) * 100
        performance_score = max(0, 100 - connect_time * 10)
        total_score = capability_score * 0.7 + performance_score * 0.3
        return ValidationResult(model_id=f'{model_info.provider}/{model_info.model_name}', is_valid=len(capabilities_passed) > 0, response_time=time.time() - start_time, capabilities_tested=capabilities_tested, capabilities_passed=capabilities_passed, score=total_score)

    @lru_cache(maxsize=128)
    def test_basic_connectivity(self, model_info) -> Tuple[bool, float, str]:
        """Test if model is reachable and responsive"""
        start_time = time.time()
        try:
            if model_info.provider == 'ollama':
                return self._test_ollama_connectivity(model_info)
            elif model_info.provider == 'huggingface':
                return self._test_huggingface_connectivity(model_info)
            elif model_info.provider == 'replicate':
                return self._test_replicate_connectivity(model_info)
            elif model_info.provider == 'together':
                return self._test_together_connectivity(model_info)
            elif model_info.provider == 'openai':
                return self._test_openai_connectivity(model_info)
            elif model_info.provider == 'anthropic':
                return self._test_anthropic_connectivity(model_info)
            elif model_info.provider == 'google':
                return self._test_google_connectivity(model_info)
            else:
                response = requests.get(model_info.api_endpoint, timeout=10)
                response_time = time.time() - start_time
                return (response.status_code == 200, response_time, '')
        except Exception as e:
            return (False, time.time() - start_time, str(e))

    def _test_ollama_connectivity(self, model_info) -> Tuple[bool, float, str]:
        """Test Ollama connectivity"""
        start_time = time.time()
        try:
            response = requests.get(f'{model_info.api_endpoint}/api/tags', timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_exists = any((m['name'] == model_info.model_name for m in models))
                response_time = time.time() - start_time
                if model_exists:
                    return (True, response_time, '')
                else:
                    return (False, response_time, f'Model {model_info.model_name} not found')
            else:
                return (False, time.time() - start_time, f'HTTP {response.status_code}')
        except Exception as e:
            return (False, time.time() - start_time, str(e))

    def _test_huggingface_connectivity(self, model_info) -> Tuple[bool, float, str]:
        """Test HuggingFace connectivity"""
        start_time = time.time()
        try:
            response = requests.get(f'https://huggingface.co/api/models/{model_info.model_name}', timeout=5)
            response_time = time.time() - start_time
            return (response.status_code == 200, response_time, '')
        except Exception as e:
            return (False, time.time() - start_time, str(e))

    def _test_replicate_connectivity(self, model_info) -> Tuple[bool, float, str]:
        """Test Replicate connectivity"""
        start_time = time.time()
        try:
            response = requests.get(f'https://replicate.com/{model_info.model_name}', timeout=5)
            response_time = time.time() - start_time
            return (response.status_code == 200, response_time, '')
        except Exception as e:
            return (False, time.time() - start_time, str(e))

    def _test_together_connectivity(self, model_info) -> Tuple[bool, float, str]:
        """Test Together.ai connectivity"""
        start_time = time.time()
        try:
            response = requests.get('https://api.together.xyz/models', timeout=5)
            response_time = time.time() - start_time
            return (response.status_code == 200, response_time, '')
        except Exception as e:
            return (False, time.time() - start_time, str(e))

    def _test_openai_connectivity(self, model_info) -> Tuple[bool, float, str]:
        """Test OpenAI connectivity"""
        start_time = time.time()
        try:
            headers = {}
            if hasattr(model_info, 'api_key') and model_info.api_key:
                headers['Authorization'] = f'Bearer {model_info.api_key}'
            response = requests.get('https://api.openai.com/v1/models', headers=headers, timeout=5)
            response_time = time.time() - start_time
            return (response.status_code == 200, response_time, '')
        except Exception as e:
            return (False, time.time() - start_time, str(e))

    def _test_anthropic_connectivity(self, model_info) -> Tuple[bool, float, str]:
        """Test Anthropic connectivity"""
        start_time = time.time()
        try:
            headers = {'anthropic-version': '2023-06-01'}
            if hasattr(model_info, 'api_key') and model_info.api_key:
                headers['x-api-key'] = model_info.api_key
            response = requests.post('https://api.anthropic.com/v1/messages', headers=headers, json={'model': model_info.model_name, 'max_tokens': 1, 'messages': []}, timeout=5)
            response_time = time.time() - start_time
            return (response.status_code in [200, 400], response_time, '')
        except Exception as e:
            return (False, time.time() - start_time, str(e))

    def _test_google_connectivity(self, model_info) -> Tuple[bool, float, str]:
        """Test Google AI connectivity"""
        start_time = time.time()
        try:
            response = requests.get('https://generativelanguage.googleapis.com/v1beta/models', timeout=5)
            response_time = time.time() - start_time
            return (response.status_code == 200, response_time, '')
        except Exception as e:
            return (False, time.time() - start_time, str(e))

    def test_capability(self, model_info, capability: str, prompt: str) -> Tuple[bool, str]:
        """Test a specific capability of the model"""
        try:
            if model_info.provider == 'ollama':
                return self._test_ollama_capability(model_info, prompt)
            else:
                return (True, 'Basic connectivity test passed')
        except Exception as e:
            logger.warning(f'Capability test failed: {e}')
            return (False, str(e))

    def _test_ollama_capability(self, model_info, prompt: str) -> Tuple[bool, str]:
        """Test Ollama model capability with actual prompt"""
        try:
            payload = {'model': model_info.model_name, 'prompt': prompt, 'max_tokens': 100, 'temperature': 0.1}
            response = requests.post(f'{model_info.api_endpoint}/api/generate', json=payload, timeout=10)
            if response.status_code == 200:
                data = response.json()
                response_text = data.get('response', '')
                return (len(response_text.strip()) > 10, response_text)
            else:
                return (False, f'HTTP {response.status_code}')
        except Exception as e:
            return (False, str(e))

    def benchmark_model(self, model_info, test_prompts: Optional[List[str]]=None) -> Dict[str, float]:
        """Benchmark model performance across multiple prompts"""
        if test_prompts is None:
            test_prompts = ['Hello, how are you?', 'What is 2+2?', 'Write a short haiku about coding.']
        results = {}
        for prompt in test_prompts:
            start_time = time.time()
            try:
                if model_info.provider == 'ollama':
                    (success, response) = self._test_ollama_capability(model_info, prompt)
                    response_time = time.time() - start_time
                    results[prompt[:30] + '...'] = response_time if success else -1
                else:
                    time.sleep(0.1)
                    results[prompt[:30] + '...'] = time.time() - start_time
            except Exception as e:
                results[prompt[:30] + '...'] = -1
        return results

    def generate_model_report(self, validation_results: List[ValidationResult]) -> str:
        """Generate a comprehensive report of model validation results"""
        report = '# AI Model Validation Report\n\n'
        report += f'## Summary\n\n'
        report += f'- Total models tested: {len(validation_results)}\n'
        report += f'- Valid models: {len([r for r in validation_results if r.is_valid])}\n'
        report += f'- Average response time: {sum((r.response_time for r in validation_results)) / len(validation_results):.2f}s\n\n'
        report += '## Model Details\n\n'
        report += '| Model | Status | Score | Response Time | Capabilities |\n'
        report += '|-------|--------|-------|---------------|--------------|\n'
        for result in sorted(validation_results, key=lambda x: x.score, reverse=True):
            status = '✅ Valid' if result.is_valid else '❌ Invalid'
            capabilities = ', '.join(result.capabilities_passed)
            report += f'| {result.model_id} | {status} | {result.score:.1f} | {result.response_time:.2f}s | {capabilities} |\n'
        report += '\n## Recommendations\n\n'
        valid_results = [r for r in validation_results if r.is_valid]
        if valid_results:
            fastest = min(valid_results, key=lambda x: x.response_time)
            report += f'- **Fastest Model**: {fastest.model_id} ({fastest.response_time:.2f}s)\n'
            best = max(valid_results, key=lambda x: x.score)
            report += f'- **Best Overall**: {best.model_id} (Score: {best.score:.1f})\n'
            most_capable = max(valid_results, key=lambda x: len(x.capabilities_passed))
            report += f'- **Most Capable**: {most_capable.model_id} ({len(most_capable.capabilities_passed)} capabilities)\n'
        return report